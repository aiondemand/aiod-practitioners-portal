{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AIOD_RAIL_API_KEY'] = '9b9c7f0cd6af7656b2b480d842d79863'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import aiod_rail_sdk\n",
    "import aiod_rail_sdk.configuration\n",
    "from client.client import RailClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = '/home/eddie/Projects/script.py'\n",
    "requirements_path = '/home/eddie/Projects/requirements.txt'\n",
    "image_path = '/home/eddie/Projects/Dockerfile'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/eddie/Projects/template.json') as f:\n",
    "  template = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = aiod_rail_sdk.configuration.Configuration(host='http://localhost/api')\n",
    "my_client = RailClient(config, api_key=os.environ.get('AIOD_RAIL_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Templates endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create experiment template from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentTemplateResponse(name='ExpTempl', description='Template description', task=<TaskType.TEXT_CLASSIFICATION: 'TEXT_CLASSIFICATION'>, datasets_schema=AssetSchema(cardinality=<AssetCardinality.ENUM_1_MINUS_1: '1-1'>), models_schema=AssetSchema(cardinality=<AssetCardinality.ENUM_1_MINUS_1: '1-1'>), envs_required=[EnvironmentVarDef(name='SPLIT_NAME', description='name of a subset')], envs_optional=[], script='import os\\n\\nos.environ[\"HF_HOME\"] = \".\"\\n\\nimport json\\nimport logging\\n\\nimport numpy as np\\nimport sklearn.metrics as m\\nimport torch\\nimport wandb\\nfrom datasets import load_dataset\\nfrom tqdm import tqdm\\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\\n\\n\\ndef get_device():\\n    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n\\n\\ndef wandb_check():\\n    necessary_envs = [\\n        \"WANDB_API_KEY\",\\n        \"WANDB_ENTITY\",\\n        \"WANDB_PROJECT\",\\n        \"WANDB_NAME\",\\n    ]\\n    return set(necessary_envs).issubset(os.environ)\\n\\n\\ndef process_metrics(all_metrics: dict, metrics_filter: list):\\n    metrics_of_interest = {k: all_metrics.get(k) for k in metrics_filter}\\n\\n    os.makedirs(\"./output-temp\", exist_ok=True)\\n    with open(\"./output-temp/metrics.json\", \"w\") as f:\\n        json.dump(metrics_of_interest, f)\\n\\n    if wandb_check():\\n        wandb.init(\\n            project=os.environ[\"WANDB_PROJECT\"],\\n            name=os.environ[\"WANDB_NAME\"],\\n            entity=os.environ[\"WANDB_ENTITY\"],\\n        )\\n        wandb.log(metrics_of_interest)\\n        wandb.finish()\\n\\n\\nif __name__ == \"__main__\":\\n    model_name = os.getenv(\"MODEL_NAMES\").split(\",\")[0]\\n    dataset_name = \"mtkinit/Example-Dataset-Super-2\"\\n    metrics_to_compute = (\\n        os.getenv(\"METRICS\", default=\"\").split(\",\")\\n        if len(os.getenv(\"METRICS\", default=\"\"))\\n        else []\\n    )\\n    split_name = os.getenv(\"SPLIT_NAME\", default=\"train\")\\n\\n    for attempt in range(5):\\n        try:\\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\\n            model = AutoModelForSequenceClassification.from_pretrained(model_name)\\n            model.to(get_device())\\n            model.eval()\\n\\n            dataset = load_dataset(dataset_name)[split_name]\\n            break\\n        except Exception:\\n            logging.getLogger().warning(\\n                f\"Failed to load model and data (attempt: {attempt})\"\\n            )\\n            continue\\n    else:\\n        logging.getLogger().error(\"Failed to load model and data.\")\\n        exit(1)\\n\\n    batch_size = 8\\n    all_predictions = []\\n    all_labels = []\\n\\n    # predicting\\n    for i in tqdm(range(0, len(dataset), batch_size)):\\n        start_idx = i\\n        end_idx = i + batch_size\\n\\n        sentences = dataset[start_idx:end_idx][\"sentence\"]\\n        labels = dataset[start_idx:end_idx][\"label\"]\\n        labels = torch.tensor(labels)\\n\\n        encoding = tokenizer(\\n            sentences, padding=True, truncation=True, return_tensors=\"pt\"\\n        )\\n        encoding = {k: v.to(get_device()) for k, v in encoding.items()}\\n\\n        with torch.no_grad():\\n            out = model(**encoding)[0]\\n\\n        pred = out.argmax(dim=1)\\n        pred = pred - 1\\n\\n        all_predictions += [pred.cpu().numpy()]\\n        all_labels += [labels.cpu().numpy()]\\n\\n    # calculating metrics\\n    all_predictions = np.hstack(all_predictions)\\n    all_labels = np.hstack(all_labels)\\n\\n    accuracy = m.accuracy_score(all_labels, all_predictions)\\n    precision_macro = m.precision_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n    recall_macro = m.recall_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n    f1_macro = m.f1_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n\\n    metrics = {\\n        \"accuracy\": accuracy,\\n        \"precision_macro\": precision_macro,\\n        \"recall_macro\": recall_macro,\\n        \"f1_macro\": f1_macro,\\n    }\\n    process_metrics(metrics, metrics_to_compute)\\n', pip_requirements='transformers==4.30.2\\ndatasets==2.14.6\\nnumpy==1.25.0\\nscikit-learn==1.2.2\\nwandb==0.15.4\\n--extra-index-url https://download.pytorch.org/whl/cpu\\ntorch==2.0.0+cpu\\n', public=True, id='66570433b986f37c731ead6c', created_at=datetime.datetime(2024, 5, 29, 10, 32, 19, 651566, tzinfo=TzInfo(UTC)), updated_at=datetime.datetime(2024, 5, 29, 10, 32, 19, 651576, tzinfo=TzInfo(UTC)), state=<TemplateState.CREATED: 'CREATED'>, dockerfile='FROM python:3.9\\nWORKDIR /app\\n\\nCOPY requirements.txt .\\nRUN pip install -r requirements.txt\\n', approved=False, archived=False, mine=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = my_client.experiments_templates.create_experiment_template(file=template)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'CreatedExperiment',\n",
       " 'description': 'DescriptionChangedToSomethingElse',\n",
       " 'task': 'TEXT_CLASSIFICATION',\n",
       " 'datasets_schema': {'cardinality': '1-1'},\n",
       " 'models_schema': {'cardinality': '1-1'},\n",
       " 'envs_required': [{'name': 'SPLIT_NAME', 'description': 'name of a subset'}],\n",
       " 'envs_optional': [],\n",
       " 'available_metrics': ['accuracy'],\n",
       " 'public': True,\n",
       " 'base_image': 'python:3.9',\n",
       " 'script': 'import os\\r\\n\\r\\nos.environ[\"HF_HOME\"] = \".\"\\r\\n\\r\\nimport json\\r\\nimport logging\\r\\n\\r\\nimport numpy as np\\r\\nimport sklearn.metrics as m\\r\\nimport torch\\r\\nimport wandb\\r\\nfrom datasets import load_dataset\\r\\nfrom tqdm import tqdm\\r\\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\\r\\n\\r\\n\\r\\ndef get_device():\\r\\n    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\\r\\n\\r\\n\\r\\ndef wandb_check():\\r\\n    necessary_envs = [\\r\\n        \"WANDB_API_KEY\",\\r\\n        \"WANDB_ENTITY\",\\r\\n        \"WANDB_PROJECT\",\\r\\n        \"WANDB_NAME\",\\r\\n    ]\\r\\n    return set(necessary_envs).issubset(os.environ)\\r\\n\\r\\n\\r\\ndef process_metrics(all_metrics: dict, metrics_filter: list):\\r\\n    metrics_of_interest = {k: all_metrics.get(k) for k in metrics_filter}\\r\\n\\r\\n    os.makedirs(\"./output-temp\", exist_ok=True)\\r\\n    with open(\"./output-temp/metrics.json\", \"w\") as f:\\r\\n        json.dump(metrics_of_interest, f)\\r\\n\\r\\n    if wandb_check():\\r\\n        wandb.init(\\r\\n            project=os.environ[\"WANDB_PROJECT\"],\\r\\n            name=os.environ[\"WANDB_NAME\"],\\r\\n            entity=os.environ[\"WANDB_ENTITY\"],\\r\\n        )\\r\\n        wandb.log(metrics_of_interest)\\r\\n        wandb.finish()\\r\\n\\r\\n\\r\\nif __name__ == \"__main__\":\\r\\n    model_name = os.getenv(\"MODEL_NAMES\").split(\",\")[0]\\r\\n    dataset_name = \"mtkinit/Example-Dataset-Super-2\"\\r\\n    metrics_to_compute = (\\r\\n        os.getenv(\"METRICS\", default=\"\").split(\",\")\\r\\n        if len(os.getenv(\"METRICS\", default=\"\"))\\r\\n        else []\\r\\n    )\\r\\n    split_name = os.getenv(\"SPLIT_NAME\", default=\"train\")\\r\\n\\r\\n    for attempt in range(5):\\r\\n        try:\\r\\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\\r\\n            model = AutoModelForSequenceClassification.from_pretrained(model_name)\\r\\n            model.to(get_device())\\r\\n            model.eval()\\r\\n\\r\\n            dataset = load_dataset(dataset_name)[split_name]\\r\\n            break\\r\\n        except Exception:\\r\\n            logging.getLogger().warning(\\r\\n                f\"Failed to load model and data (attempt: {attempt})\"\\r\\n            )\\r\\n            continue\\r\\n    else:\\r\\n        logging.getLogger().error(\"Failed to load model and data.\")\\r\\n        exit(1)\\r\\n\\r\\n    batch_size = 8\\r\\n    all_predictions = []\\r\\n    all_labels = []\\r\\n\\r\\n    # predicting\\r\\n    for i in tqdm(range(0, len(dataset), batch_size)):\\r\\n        start_idx = i\\r\\n        end_idx = i + batch_size\\r\\n\\r\\n        sentences = dataset[start_idx:end_idx][\"sentence\"]\\r\\n        labels = dataset[start_idx:end_idx][\"label\"]\\r\\n        labels = torch.tensor(labels)\\r\\n\\r\\n        encoding = tokenizer(\\r\\n            sentences, padding=True, truncation=True, return_tensors=\"pt\"\\r\\n        )\\r\\n        encoding = {k: v.to(get_device()) for k, v in encoding.items()}\\r\\n\\r\\n        with torch.no_grad():\\r\\n            out = model(**encoding)[0]\\r\\n\\r\\n        pred = out.argmax(dim=1)\\r\\n        pred = pred - 1\\r\\n\\r\\n        all_predictions += [pred.cpu().numpy()]\\r\\n        all_labels += [labels.cpu().numpy()]\\r\\n\\r\\n    # calculating metrics\\r\\n    all_predictions = np.hstack(all_predictions)\\r\\n    all_labels = np.hstack(all_labels)\\r\\n\\r\\n    accuracy = m.accuracy_score(all_labels, all_predictions)\\r\\n    precision_macro = m.precision_score(\\r\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\r\\n    )\\r\\n    recall_macro = m.recall_score(\\r\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\r\\n    )\\r\\n    f1_macro = m.f1_score(\\r\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\r\\n    )\\r\\n\\r\\n    metrics = {\\r\\n        \"accuracy\": accuracy,\\r\\n        \"precision_macro\": precision_macro,\\r\\n        \"recall_macro\": recall_macro,\\r\\n        \"f1_macro\": f1_macro,\\r\\n    }\\r\\n    process_metrics(metrics, metrics_to_compute)\\r\\n',\n",
       " 'pip_requirements': 'transformers==4.30.2\\ndatasets==2.14.6\\nnumpy==1.25.0\\nscikit-learn==1.2.2\\nwandb==0.15.4\\n--extra-index-url https://download.pytorch.org/whl/cpu\\ntorch==2.0.0+cpu\\n'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_changed = template.copy()\n",
    "template_changed['name'] = 'CreatedExperiment'\n",
    "template_changed['description'] = 'DescriptionChangedToSomethingElse'\n",
    "template_changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the experiment templates, which are not approved and not built and take the first one from them to select it for update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 6655b12972f875012fe205f5,\n",
      "template: {'name': 'ExpTempl', 'description': 'Template description', 'task': <TaskType.TEXT_CLASSIFICATION: 'TEXT_CLASSIFICATION'>, 'datasets_schema': {'cardinality': <AssetCardinality.ENUM_1_MINUS_1: '1-1'>}, 'models_schema': {'cardinality': <AssetCardinality.ENUM_1_MINUS_1: '1-1'>}, 'envs_required': [{'name': 'SPLIT_NAME', 'description': 'name of a subset'}], 'envs_optional': [], 'script': 'import os\\n\\nos.environ[\"HF_HOME\"] = \".\"\\n\\nimport json\\nimport logging\\n\\nimport numpy as np\\nimport sklearn.metrics as m\\nimport torch\\nimport wandb\\nfrom datasets import load_dataset\\nfrom tqdm import tqdm\\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\\n\\n\\ndef get_device():\\n    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n\\n\\ndef wandb_check():\\n    necessary_envs = [\\n        \"WANDB_API_KEY\",\\n        \"WANDB_ENTITY\",\\n        \"WANDB_PROJECT\",\\n        \"WANDB_NAME\",\\n    ]\\n    return set(necessary_envs).issubset(os.environ)\\n\\n\\ndef process_metrics(all_metrics: dict, metrics_filter: list):\\n    metrics_of_interest = {k: all_metrics.get(k) for k in metrics_filter}\\n\\n    os.makedirs(\"./output-temp\", exist_ok=True)\\n    with open(\"./output-temp/metrics.json\", \"w\") as f:\\n        json.dump(metrics_of_interest, f)\\n\\n    if wandb_check():\\n        wandb.init(\\n            project=os.environ[\"WANDB_PROJECT\"],\\n            name=os.environ[\"WANDB_NAME\"],\\n            entity=os.environ[\"WANDB_ENTITY\"],\\n        )\\n        wandb.log(metrics_of_interest)\\n        wandb.finish()\\n\\n\\nif __name__ == \"__main__\":\\n    model_name = os.getenv(\"MODEL_NAMES\").split(\",\")[0]\\n    dataset_name = \"mtkinit/Example-Dataset-Super-2\"\\n    metrics_to_compute = (\\n        os.getenv(\"METRICS\", default=\"\").split(\",\")\\n        if len(os.getenv(\"METRICS\", default=\"\"))\\n        else []\\n    )\\n    split_name = os.getenv(\"SPLIT_NAME\", default=\"train\")\\n\\n    for attempt in range(5):\\n        try:\\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\\n            model = AutoModelForSequenceClassification.from_pretrained(model_name)\\n            model.to(get_device())\\n            model.eval()\\n\\n            dataset = load_dataset(dataset_name)[split_name]\\n            break\\n        except Exception:\\n            logging.getLogger().warning(\\n                f\"Failed to load model and data (attempt: {attempt})\"\\n            )\\n            continue\\n    else:\\n        logging.getLogger().error(\"Failed to load model and data.\")\\n        exit(1)\\n\\n    batch_size = 8\\n    all_predictions = []\\n    all_labels = []\\n\\n    # predicting\\n    for i in tqdm(range(0, len(dataset), batch_size)):\\n        start_idx = i\\n        end_idx = i + batch_size\\n\\n        sentences = dataset[start_idx:end_idx][\"sentence\"]\\n        labels = dataset[start_idx:end_idx][\"label\"]\\n        labels = torch.tensor(labels)\\n\\n        encoding = tokenizer(\\n            sentences, padding=True, truncation=True, return_tensors=\"pt\"\\n        )\\n        encoding = {k: v.to(get_device()) for k, v in encoding.items()}\\n\\n        with torch.no_grad():\\n            out = model(**encoding)[0]\\n\\n        pred = out.argmax(dim=1)\\n        pred = pred - 1\\n\\n        all_predictions += [pred.cpu().numpy()]\\n        all_labels += [labels.cpu().numpy()]\\n\\n    # calculating metrics\\n    all_predictions = np.hstack(all_predictions)\\n    all_labels = np.hstack(all_labels)\\n\\n    accuracy = m.accuracy_score(all_labels, all_predictions)\\n    precision_macro = m.precision_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n    recall_macro = m.recall_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n    f1_macro = m.f1_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n\\n    metrics = {\\n        \"accuracy\": accuracy,\\n        \"precision_macro\": precision_macro,\\n        \"recall_macro\": recall_macro,\\n        \"f1_macro\": f1_macro,\\n    }\\n    process_metrics(metrics, metrics_to_compute)\\n', 'pip_requirements': 'transformers==4.30.2\\ndatasets==2.14.6\\nnumpy==1.25.0\\nscikit-learn==1.2.2\\nwandb==0.15.4\\n--extra-index-url https://download.pytorch.org/whl/cpu\\ntorch==2.0.0+cpu\\n', 'public': True, 'id': '6655b12972f875012fe205f5', 'created_at': datetime.datetime(2024, 5, 28, 10, 25, 45, 71000), 'updated_at': datetime.datetime(2024, 5, 28, 10, 25, 45, 71000), 'state': <TemplateState.CREATED: 'CREATED'>, 'dockerfile': 'FROM python:3.9\\nWORKDIR /app\\n\\nCOPY requirements.txt .\\nRUN pip install -r requirements.txt\\n', 'approved': False, 'archived': False, 'mine': True}\n"
     ]
    }
   ],
   "source": [
    "resp = my_client.experiments_templates.get(finalized=False, approved=False)\n",
    "templates = [i.model_dump() for i in resp]\n",
    "experiment_template_id = templates[0]['id']\n",
    "print(f'id: {experiment_template_id},\\ntemplate: {templates[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update the selected experiment template, we will check the experiment id to verify that it's the same one we updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 6655b12972f875012fe205f5,\n",
      "template: name='CreatedExperiment' description='DescriptionChangedToSomethingElse' task=<TaskType.TEXT_CLASSIFICATION: 'TEXT_CLASSIFICATION'> datasets_schema=AssetSchema(cardinality=<AssetCardinality.ENUM_1_MINUS_1: '1-1'>) models_schema=AssetSchema(cardinality=<AssetCardinality.ENUM_1_MINUS_1: '1-1'>) envs_required=[EnvironmentVarDef(name='SPLIT_NAME', description='name of a subset')] envs_optional=[] script='import os\\n\\nos.environ[\"HF_HOME\"] = \".\"\\n\\nimport json\\nimport logging\\n\\nimport numpy as np\\nimport sklearn.metrics as m\\nimport torch\\nimport wandb\\nfrom datasets import load_dataset\\nfrom tqdm import tqdm\\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\\n\\n\\ndef get_device():\\n    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n\\n\\ndef wandb_check():\\n    necessary_envs = [\\n        \"WANDB_API_KEY\",\\n        \"WANDB_ENTITY\",\\n        \"WANDB_PROJECT\",\\n        \"WANDB_NAME\",\\n    ]\\n    return set(necessary_envs).issubset(os.environ)\\n\\n\\ndef process_metrics(all_metrics: dict, metrics_filter: list):\\n    metrics_of_interest = {k: all_metrics.get(k) for k in metrics_filter}\\n\\n    os.makedirs(\"./output-temp\", exist_ok=True)\\n    with open(\"./output-temp/metrics.json\", \"w\") as f:\\n        json.dump(metrics_of_interest, f)\\n\\n    if wandb_check():\\n        wandb.init(\\n            project=os.environ[\"WANDB_PROJECT\"],\\n            name=os.environ[\"WANDB_NAME\"],\\n            entity=os.environ[\"WANDB_ENTITY\"],\\n        )\\n        wandb.log(metrics_of_interest)\\n        wandb.finish()\\n\\n\\nif __name__ == \"__main__\":\\n    model_name = os.getenv(\"MODEL_NAMES\").split(\",\")[0]\\n    dataset_name = \"mtkinit/Example-Dataset-Super-2\"\\n    metrics_to_compute = (\\n        os.getenv(\"METRICS\", default=\"\").split(\",\")\\n        if len(os.getenv(\"METRICS\", default=\"\"))\\n        else []\\n    )\\n    split_name = os.getenv(\"SPLIT_NAME\", default=\"train\")\\n\\n    for attempt in range(5):\\n        try:\\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\\n            model = AutoModelForSequenceClassification.from_pretrained(model_name)\\n            model.to(get_device())\\n            model.eval()\\n\\n            dataset = load_dataset(dataset_name)[split_name]\\n            break\\n        except Exception:\\n            logging.getLogger().warning(\\n                f\"Failed to load model and data (attempt: {attempt})\"\\n            )\\n            continue\\n    else:\\n        logging.getLogger().error(\"Failed to load model and data.\")\\n        exit(1)\\n\\n    batch_size = 8\\n    all_predictions = []\\n    all_labels = []\\n\\n    # predicting\\n    for i in tqdm(range(0, len(dataset), batch_size)):\\n        start_idx = i\\n        end_idx = i + batch_size\\n\\n        sentences = dataset[start_idx:end_idx][\"sentence\"]\\n        labels = dataset[start_idx:end_idx][\"label\"]\\n        labels = torch.tensor(labels)\\n\\n        encoding = tokenizer(\\n            sentences, padding=True, truncation=True, return_tensors=\"pt\"\\n        )\\n        encoding = {k: v.to(get_device()) for k, v in encoding.items()}\\n\\n        with torch.no_grad():\\n            out = model(**encoding)[0]\\n\\n        pred = out.argmax(dim=1)\\n        pred = pred - 1\\n\\n        all_predictions += [pred.cpu().numpy()]\\n        all_labels += [labels.cpu().numpy()]\\n\\n    # calculating metrics\\n    all_predictions = np.hstack(all_predictions)\\n    all_labels = np.hstack(all_labels)\\n\\n    accuracy = m.accuracy_score(all_labels, all_predictions)\\n    precision_macro = m.precision_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n    recall_macro = m.recall_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n    f1_macro = m.f1_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n\\n    metrics = {\\n        \"accuracy\": accuracy,\\n        \"precision_macro\": precision_macro,\\n        \"recall_macro\": recall_macro,\\n        \"f1_macro\": f1_macro,\\n    }\\n    process_metrics(metrics, metrics_to_compute)\\n' pip_requirements='transformers==4.30.2\\ndatasets==2.14.6\\nnumpy==1.25.0\\nscikit-learn==1.2.2\\nwandb==0.15.4\\n--extra-index-url https://download.pytorch.org/whl/cpu\\ntorch==2.0.0+cpu\\n' public=True id='6655b12972f875012fe205f5' created_at=datetime.datetime(2024, 5, 28, 10, 25, 45, 71000) updated_at=datetime.datetime(2024, 5, 29, 10, 46, 7, 72800, tzinfo=TzInfo(UTC)) state=<TemplateState.CREATED: 'CREATED'> dockerfile='FROM python:3.9\\nWORKDIR /app\\n\\nCOPY requirements.txt .\\nRUN pip install -r requirements.txt\\n' approved=False archived=False mine=True\n"
     ]
    }
   ],
   "source": [
    "updated_experiment_template = my_client.experiments_templates.update(experiment_template_id, file=template_changed)\n",
    "id = dict(updated_experiment_template)['id']\n",
    "print(f'id: {id},\\ntemplate: {updated_experiment_template}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize experiment which are not approved and not built again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('6655b12972f875012fe205f5', 'CreatedExperiment'),\n",
       " ('6655b24a72f875012fe205f6', 'ExperimentChanged'),\n",
       " ('66570433b986f37c731ead6c', 'ExpTempl')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = my_client.experiments_templates.get(finalized=False, approved=False)\n",
    "templates = [i.model_dump() for i in resp]\n",
    "experiments = [(i['id'], i['name']) for i in templates]\n",
    "experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the last from these experiments and approve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_client.experiments_templates.approve_experiment_template(experiments[2][0], is_approved=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the updated experiment temlpate has dissappeared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'CreatedExperiment', 'description': 'DescriptionChangedToSomethingElse', 'task': <TaskType.TEXT_CLASSIFICATION: 'TEXT_CLASSIFICATION'>, 'datasets_schema': {'cardinality': <AssetCardinality.ENUM_1_MINUS_1: '1-1'>}, 'models_schema': {'cardinality': <AssetCardinality.ENUM_1_MINUS_1: '1-1'>}, 'envs_required': [{'name': 'SPLIT_NAME', 'description': 'name of a subset'}], 'envs_optional': [], 'script': 'import os\\n\\nos.environ[\"HF_HOME\"] = \".\"\\n\\nimport json\\nimport logging\\n\\nimport numpy as np\\nimport sklearn.metrics as m\\nimport torch\\nimport wandb\\nfrom datasets import load_dataset\\nfrom tqdm import tqdm\\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\\n\\n\\ndef get_device():\\n    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n\\n\\ndef wandb_check():\\n    necessary_envs = [\\n        \"WANDB_API_KEY\",\\n        \"WANDB_ENTITY\",\\n        \"WANDB_PROJECT\",\\n        \"WANDB_NAME\",\\n    ]\\n    return set(necessary_envs).issubset(os.environ)\\n\\n\\ndef process_metrics(all_metrics: dict, metrics_filter: list):\\n    metrics_of_interest = {k: all_metrics.get(k) for k in metrics_filter}\\n\\n    os.makedirs(\"./output-temp\", exist_ok=True)\\n    with open(\"./output-temp/metrics.json\", \"w\") as f:\\n        json.dump(metrics_of_interest, f)\\n\\n    if wandb_check():\\n        wandb.init(\\n            project=os.environ[\"WANDB_PROJECT\"],\\n            name=os.environ[\"WANDB_NAME\"],\\n            entity=os.environ[\"WANDB_ENTITY\"],\\n        )\\n        wandb.log(metrics_of_interest)\\n        wandb.finish()\\n\\n\\nif __name__ == \"__main__\":\\n    model_name = os.getenv(\"MODEL_NAMES\").split(\",\")[0]\\n    dataset_name = \"mtkinit/Example-Dataset-Super-2\"\\n    metrics_to_compute = (\\n        os.getenv(\"METRICS\", default=\"\").split(\",\")\\n        if len(os.getenv(\"METRICS\", default=\"\"))\\n        else []\\n    )\\n    split_name = os.getenv(\"SPLIT_NAME\", default=\"train\")\\n\\n    for attempt in range(5):\\n        try:\\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\\n            model = AutoModelForSequenceClassification.from_pretrained(model_name)\\n            model.to(get_device())\\n            model.eval()\\n\\n            dataset = load_dataset(dataset_name)[split_name]\\n            break\\n        except Exception:\\n            logging.getLogger().warning(\\n                f\"Failed to load model and data (attempt: {attempt})\"\\n            )\\n            continue\\n    else:\\n        logging.getLogger().error(\"Failed to load model and data.\")\\n        exit(1)\\n\\n    batch_size = 8\\n    all_predictions = []\\n    all_labels = []\\n\\n    # predicting\\n    for i in tqdm(range(0, len(dataset), batch_size)):\\n        start_idx = i\\n        end_idx = i + batch_size\\n\\n        sentences = dataset[start_idx:end_idx][\"sentence\"]\\n        labels = dataset[start_idx:end_idx][\"label\"]\\n        labels = torch.tensor(labels)\\n\\n        encoding = tokenizer(\\n            sentences, padding=True, truncation=True, return_tensors=\"pt\"\\n        )\\n        encoding = {k: v.to(get_device()) for k, v in encoding.items()}\\n\\n        with torch.no_grad():\\n            out = model(**encoding)[0]\\n\\n        pred = out.argmax(dim=1)\\n        pred = pred - 1\\n\\n        all_predictions += [pred.cpu().numpy()]\\n        all_labels += [labels.cpu().numpy()]\\n\\n    # calculating metrics\\n    all_predictions = np.hstack(all_predictions)\\n    all_labels = np.hstack(all_labels)\\n\\n    accuracy = m.accuracy_score(all_labels, all_predictions)\\n    precision_macro = m.precision_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n    recall_macro = m.recall_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n    f1_macro = m.f1_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n\\n    metrics = {\\n        \"accuracy\": accuracy,\\n        \"precision_macro\": precision_macro,\\n        \"recall_macro\": recall_macro,\\n        \"f1_macro\": f1_macro,\\n    }\\n    process_metrics(metrics, metrics_to_compute)\\n', 'pip_requirements': 'transformers==4.30.2\\ndatasets==2.14.6\\nnumpy==1.25.0\\nscikit-learn==1.2.2\\nwandb==0.15.4\\n--extra-index-url https://download.pytorch.org/whl/cpu\\ntorch==2.0.0+cpu\\n', 'public': True, 'id': '6655b12972f875012fe205f5', 'created_at': datetime.datetime(2024, 5, 28, 10, 25, 45, 71000), 'updated_at': datetime.datetime(2024, 5, 29, 10, 58, 20, 470000), 'state': <TemplateState.CREATED: 'CREATED'>, 'dockerfile': 'FROM python:3.9\\nWORKDIR /app\\n\\nCOPY requirements.txt .\\nRUN pip install -r requirements.txt\\n', 'approved': False, 'archived': False, 'mine': True}\n",
      "{'name': 'ExperimentChanged', 'description': 'DescriptionChanged', 'task': <TaskType.TEXT_CLASSIFICATION: 'TEXT_CLASSIFICATION'>, 'datasets_schema': {'cardinality': <AssetCardinality.ENUM_1_MINUS_1: '1-1'>}, 'models_schema': {'cardinality': <AssetCardinality.ENUM_1_MINUS_1: '1-1'>}, 'envs_required': [{'name': 'SPLIT_NAME', 'description': 'name of a subset'}], 'envs_optional': [], 'script': 'import os\\n\\nos.environ[\"HF_HOME\"] = \".\"\\n\\nimport json\\nimport logging\\n\\nimport numpy as np\\nimport sklearn.metrics as m\\nimport torch\\nimport wandb\\nfrom datasets import load_dataset\\nfrom tqdm import tqdm\\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\\n\\n\\ndef get_device():\\n    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n\\n\\ndef wandb_check():\\n    necessary_envs = [\\n        \"WANDB_API_KEY\",\\n        \"WANDB_ENTITY\",\\n        \"WANDB_PROJECT\",\\n        \"WANDB_NAME\",\\n    ]\\n    return set(necessary_envs).issubset(os.environ)\\n\\n\\ndef process_metrics(all_metrics: dict, metrics_filter: list):\\n    metrics_of_interest = {k: all_metrics.get(k) for k in metrics_filter}\\n\\n    os.makedirs(\"./output-temp\", exist_ok=True)\\n    with open(\"./output-temp/metrics.json\", \"w\") as f:\\n        json.dump(metrics_of_interest, f)\\n\\n    if wandb_check():\\n        wandb.init(\\n            project=os.environ[\"WANDB_PROJECT\"],\\n            name=os.environ[\"WANDB_NAME\"],\\n            entity=os.environ[\"WANDB_ENTITY\"],\\n        )\\n        wandb.log(metrics_of_interest)\\n        wandb.finish()\\n\\n\\nif __name__ == \"__main__\":\\n    model_name = os.getenv(\"MODEL_NAMES\").split(\",\")[0]\\n    dataset_name = \"mtkinit/Example-Dataset-Super-2\"\\n    metrics_to_compute = (\\n        os.getenv(\"METRICS\", default=\"\").split(\",\")\\n        if len(os.getenv(\"METRICS\", default=\"\"))\\n        else []\\n    )\\n    split_name = os.getenv(\"SPLIT_NAME\", default=\"train\")\\n\\n    for attempt in range(5):\\n        try:\\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\\n            model = AutoModelForSequenceClassification.from_pretrained(model_name)\\n            model.to(get_device())\\n            model.eval()\\n\\n            dataset = load_dataset(dataset_name)[split_name]\\n            break\\n        except Exception:\\n            logging.getLogger().warning(\\n                f\"Failed to load model and data (attempt: {attempt})\"\\n            )\\n            continue\\n    else:\\n        logging.getLogger().error(\"Failed to load model and data.\")\\n        exit(1)\\n\\n    batch_size = 8\\n    all_predictions = []\\n    all_labels = []\\n\\n    # predicting\\n    for i in tqdm(range(0, len(dataset), batch_size)):\\n        start_idx = i\\n        end_idx = i + batch_size\\n\\n        sentences = dataset[start_idx:end_idx][\"sentence\"]\\n        labels = dataset[start_idx:end_idx][\"label\"]\\n        labels = torch.tensor(labels)\\n\\n        encoding = tokenizer(\\n            sentences, padding=True, truncation=True, return_tensors=\"pt\"\\n        )\\n        encoding = {k: v.to(get_device()) for k, v in encoding.items()}\\n\\n        with torch.no_grad():\\n            out = model(**encoding)[0]\\n\\n        pred = out.argmax(dim=1)\\n        pred = pred - 1\\n\\n        all_predictions += [pred.cpu().numpy()]\\n        all_labels += [labels.cpu().numpy()]\\n\\n    # calculating metrics\\n    all_predictions = np.hstack(all_predictions)\\n    all_labels = np.hstack(all_labels)\\n\\n    accuracy = m.accuracy_score(all_labels, all_predictions)\\n    precision_macro = m.precision_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n    recall_macro = m.recall_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n    f1_macro = m.f1_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n\\n    metrics = {\\n        \"accuracy\": accuracy,\\n        \"precision_macro\": precision_macro,\\n        \"recall_macro\": recall_macro,\\n        \"f1_macro\": f1_macro,\\n    }\\n    process_metrics(metrics, metrics_to_compute)\\n', 'pip_requirements': 'transformers==4.30.2\\ndatasets==2.14.6\\nnumpy==1.25.0\\nscikit-learn==1.2.2\\nwandb==0.15.4\\n--extra-index-url https://download.pytorch.org/whl/cpu\\ntorch==2.0.0+cpu\\n', 'public': True, 'id': '6655b24a72f875012fe205f6', 'created_at': datetime.datetime(2024, 5, 28, 10, 30, 34, 978000), 'updated_at': datetime.datetime(2024, 5, 28, 11, 32, 24, 822000), 'state': <TemplateState.CREATED: 'CREATED'>, 'dockerfile': 'FROM python:3.9\\nWORKDIR /app\\n\\nCOPY requirements.txt .\\nRUN pip install -r requirements.txt\\n', 'approved': False, 'archived': False, 'mine': True}\n"
     ]
    }
   ],
   "source": [
    "response = my_client.experiments_templates.get(finalized=False, approved=False)\n",
    "rest_of_e_t = [i.model_dump() for i in response]\n",
    "for t in rest_of_e_t:\n",
    "    if t['approved'] == False:\n",
    "        print(f'{t}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the count of experiment templates which are not approved and not built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.experiments_templates.count(finalized=False, approved=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the first experiment by it's id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentTemplateResponse(name='CreatedExperiment', description='DescriptionChangedToSomethingElse', task=<TaskType.TEXT_CLASSIFICATION: 'TEXT_CLASSIFICATION'>, datasets_schema=AssetSchema(cardinality=<AssetCardinality.ENUM_1_MINUS_1: '1-1'>), models_schema=AssetSchema(cardinality=<AssetCardinality.ENUM_1_MINUS_1: '1-1'>), envs_required=[EnvironmentVarDef(name='SPLIT_NAME', description='name of a subset')], envs_optional=[], script='import os\\n\\nos.environ[\"HF_HOME\"] = \".\"\\n\\nimport json\\nimport logging\\n\\nimport numpy as np\\nimport sklearn.metrics as m\\nimport torch\\nimport wandb\\nfrom datasets import load_dataset\\nfrom tqdm import tqdm\\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\\n\\n\\ndef get_device():\\n    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n\\n\\ndef wandb_check():\\n    necessary_envs = [\\n        \"WANDB_API_KEY\",\\n        \"WANDB_ENTITY\",\\n        \"WANDB_PROJECT\",\\n        \"WANDB_NAME\",\\n    ]\\n    return set(necessary_envs).issubset(os.environ)\\n\\n\\ndef process_metrics(all_metrics: dict, metrics_filter: list):\\n    metrics_of_interest = {k: all_metrics.get(k) for k in metrics_filter}\\n\\n    os.makedirs(\"./output-temp\", exist_ok=True)\\n    with open(\"./output-temp/metrics.json\", \"w\") as f:\\n        json.dump(metrics_of_interest, f)\\n\\n    if wandb_check():\\n        wandb.init(\\n            project=os.environ[\"WANDB_PROJECT\"],\\n            name=os.environ[\"WANDB_NAME\"],\\n            entity=os.environ[\"WANDB_ENTITY\"],\\n        )\\n        wandb.log(metrics_of_interest)\\n        wandb.finish()\\n\\n\\nif __name__ == \"__main__\":\\n    model_name = os.getenv(\"MODEL_NAMES\").split(\",\")[0]\\n    dataset_name = \"mtkinit/Example-Dataset-Super-2\"\\n    metrics_to_compute = (\\n        os.getenv(\"METRICS\", default=\"\").split(\",\")\\n        if len(os.getenv(\"METRICS\", default=\"\"))\\n        else []\\n    )\\n    split_name = os.getenv(\"SPLIT_NAME\", default=\"train\")\\n\\n    for attempt in range(5):\\n        try:\\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\\n            model = AutoModelForSequenceClassification.from_pretrained(model_name)\\n            model.to(get_device())\\n            model.eval()\\n\\n            dataset = load_dataset(dataset_name)[split_name]\\n            break\\n        except Exception:\\n            logging.getLogger().warning(\\n                f\"Failed to load model and data (attempt: {attempt})\"\\n            )\\n            continue\\n    else:\\n        logging.getLogger().error(\"Failed to load model and data.\")\\n        exit(1)\\n\\n    batch_size = 8\\n    all_predictions = []\\n    all_labels = []\\n\\n    # predicting\\n    for i in tqdm(range(0, len(dataset), batch_size)):\\n        start_idx = i\\n        end_idx = i + batch_size\\n\\n        sentences = dataset[start_idx:end_idx][\"sentence\"]\\n        labels = dataset[start_idx:end_idx][\"label\"]\\n        labels = torch.tensor(labels)\\n\\n        encoding = tokenizer(\\n            sentences, padding=True, truncation=True, return_tensors=\"pt\"\\n        )\\n        encoding = {k: v.to(get_device()) for k, v in encoding.items()}\\n\\n        with torch.no_grad():\\n            out = model(**encoding)[0]\\n\\n        pred = out.argmax(dim=1)\\n        pred = pred - 1\\n\\n        all_predictions += [pred.cpu().numpy()]\\n        all_labels += [labels.cpu().numpy()]\\n\\n    # calculating metrics\\n    all_predictions = np.hstack(all_predictions)\\n    all_labels = np.hstack(all_labels)\\n\\n    accuracy = m.accuracy_score(all_labels, all_predictions)\\n    precision_macro = m.precision_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n    recall_macro = m.recall_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n    f1_macro = m.f1_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n\\n    metrics = {\\n        \"accuracy\": accuracy,\\n        \"precision_macro\": precision_macro,\\n        \"recall_macro\": recall_macro,\\n        \"f1_macro\": f1_macro,\\n    }\\n    process_metrics(metrics, metrics_to_compute)\\n', pip_requirements='transformers==4.30.2\\ndatasets==2.14.6\\nnumpy==1.25.0\\nscikit-learn==1.2.2\\nwandb==0.15.4\\n--extra-index-url https://download.pytorch.org/whl/cpu\\ntorch==2.0.0+cpu\\n', public=True, id='6655b12972f875012fe205f5', created_at=datetime.datetime(2024, 5, 28, 10, 25, 45, 71000), updated_at=datetime.datetime(2024, 5, 29, 10, 58, 20, 470000), state=<TemplateState.CREATED: 'CREATED'>, dockerfile='FROM python:3.9\\nWORKDIR /app\\n\\nCOPY requirements.txt .\\nRUN pip install -r requirements.txt\\n', approved=False, archived=False, mine=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_t = rest_of_e_t[0]\n",
    "my_client.experiments_templates.get_by_id(e_t['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will archive this experiment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_client.experiments_templates.archive(e_t['id'], archived=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look again on count and list of experiments which are not approved and not built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.experiments_templates.count(finalized=False, approved=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: CreatedExperiment ID: 6655b12972f875012fe205f5\n",
      "Archived: True\n",
      "Name: ExperimentChanged ID: 6655b24a72f875012fe205f6\n",
      "Archived: False\n"
     ]
    }
   ],
   "source": [
    "resp = my_client.experiments_templates.get(finalized=False, approved=False)\n",
    "templates = [i.model_dump() for i in resp]\n",
    "for t in templates:\n",
    "    print(f\"Name: {t['name']} ID: {t['id']}\\nArchived: {t['archived']}\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view number of datasets we can browse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411932"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.datasets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get two of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset(platform='huggingface', platform_resource_identifier='acronym_identification', name='acronym_identification', date_published=datetime.datetime(2022, 3, 2, 23, 29, 22), same_as='https://huggingface.co/datasets/acronym_identification', is_accessible_for_free=True, version=None, issn=None, measurement_technique=None, temporal_coverage=None, ai_asset_identifier=2, ai_resource_identifier=2, aiod_entry=AIoDEntryRead(editor=[], status='published', date_modified=datetime.datetime(2023, 12, 19, 10, 25, 29), date_created=datetime.datetime(2023, 12, 19, 10, 25, 29)), alternate_name=[], application_area=[], citation=[], contact=[], creator=[], description=Text(plain='Acronym identification training and development sets for the acronym identification task at SDU@AAAI-21.', html=None), distribution=[Distribution(platform=None, platform_resource_identifier=None, checksum=None, checksum_algorithm=None, copyright=None, content_url='https://huggingface.co/datasets/acronym_identification/resolve/refs%2Fconvert%2Fparquet/default/test/0000.parquet', content_size_kb=206100, date_published=None, description='acronym_identification. Config: default. Split: test', encoding_format=None, name='0000.parquet', technology_readiness_level=None), Distribution(platform=None, platform_resource_identifier=None, checksum=None, checksum_algorithm=None, copyright=None, content_url='https://huggingface.co/datasets/acronym_identification/resolve/refs%2Fconvert%2Fparquet/default/train/0000.parquet', content_size_kb=1657991, date_published=None, description='acronym_identification. Config: default. Split: train', encoding_format=None, name='0000.parquet', technology_readiness_level=None), Distribution(platform=None, platform_resource_identifier=None, checksum=None, checksum_algorithm=None, copyright=None, content_url='https://huggingface.co/datasets/acronym_identification/resolve/refs%2Fconvert%2Fparquet/default/validation/0000.parquet', content_size_kb=206916, date_published=None, description='acronym_identification. Config: default. Split: validation', encoding_format=None, name='0000.parquet', technology_readiness_level=None)], funder=[], has_part=[], industrial_sector=[], is_part_of=[], keyword=['source_datasets:original', 'task_categories:token-classification', 'multilinguality:monolingual', 'size_categories:10k<n<100k', 'language:en', 'acronym-identification', 'region:us', 'language_creators:found', 'annotations_creators:expert-generated', 'arxiv:2010.14678', 'license:mit'], license='mit', media=[], note=[], relevant_link=[], relevant_resource=[], relevant_to=[], research_area=[], scientific_domain=[], size=None, spatial_coverage=None, identifier=1),\n",
       " Dataset(platform='huggingface', platform_resource_identifier='ade_corpus_v2', name='ade_corpus_v2', date_published=datetime.datetime(2022, 3, 2, 23, 29, 22), same_as='https://huggingface.co/datasets/ade_corpus_v2', is_accessible_for_free=True, version=None, issn=None, measurement_technique=None, temporal_coverage=None, ai_asset_identifier=3, ai_resource_identifier=3, aiod_entry=AIoDEntryRead(editor=[], status='published', date_modified=datetime.datetime(2023, 12, 19, 10, 25, 29), date_created=datetime.datetime(2023, 12, 19, 10, 25, 29)), alternate_name=[], application_area=[], citation=[], contact=[], creator=[], description=Text(plain=' ADE-Corpus-V2  Dataset: Adverse Drug Reaction Data.\\n This is a dataset for Classification if a sentence is ADE-related (True) or not (False) and Relation Extraction between Adverse Drug Event and Drug.\\n DRUG-AE.rel provides relations between drugs and adverse effects.\\n DRUG-DOSE.rel provides relations between drugs and dosages.\\n ADE-NEG.txt provides all sentences in the ADE corpus that DO NOT contain any drug-related adverse effects.', html=None), distribution=[Distribution(platform=None, platform_resource_identifier=None, checksum=None, checksum_algorithm=None, copyright=None, content_url='https://huggingface.co/datasets/ade_corpus_v2/resolve/refs%2Fconvert%2Fparquet/Ade_corpus_v2_classification/train/0000.parquet', content_size_kb=1706476, date_published=None, description='ade_corpus_v2. Config: Ade_corpus_v2_classification. Split: train', encoding_format=None, name='0000.parquet', technology_readiness_level=None), Distribution(platform=None, platform_resource_identifier=None, checksum=None, checksum_algorithm=None, copyright=None, content_url='https://huggingface.co/datasets/ade_corpus_v2/resolve/refs%2Fconvert%2Fparquet/Ade_corpus_v2_drug_ade_relation/train/0000.parquet', content_size_kb=491362, date_published=None, description='ade_corpus_v2. Config: Ade_corpus_v2_drug_ade_relation. Split: train', encoding_format=None, name='0000.parquet', technology_readiness_level=None), Distribution(platform=None, platform_resource_identifier=None, checksum=None, checksum_algorithm=None, copyright=None, content_url='https://huggingface.co/datasets/ade_corpus_v2/resolve/refs%2Fconvert%2Fparquet/Ade_corpus_v2_drug_dosage_relation/train/0000.parquet', content_size_kb=33004, date_published=None, description='ade_corpus_v2. Config: Ade_corpus_v2_drug_dosage_relation. Split: train', encoding_format=None, name='0000.parquet', technology_readiness_level=None)], funder=[], has_part=[], industrial_sector=[], is_part_of=[], keyword=['source_datasets:original', 'task_categories:token-classification', 'multilinguality:monolingual', 'size_categories:10k<n<100k', 'language:en', 'region:us', 'language_creators:found', 'annotations_creators:expert-generated', 'task_categories:text-classification', 'size_categories:n<1k', 'size_categories:1k<n<10k', 'license:unknown', 'task_ids:fact-checking', 'task_ids:coreference-resolution'], license='unknown', media=[], note=[], relevant_link=[], relevant_resource=[], relevant_to=[], research_area=[], scientific_domain=[], size=None, spatial_coverage=None, identifier=2)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.datasets.get(offset=0, limit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load json for experiment to create in this json it is important to change the id of template to experiment template id from which we want to create the experiment,\n",
    "We will select the one which we approved => ('66570433b986f37c731ead6c', 'ExpTempl'), we can change it right here in code as needed, let's also change it's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Exp01', 'description': 'Exp description', 'publication_ids': [], 'experiment_template_id': '6655ae4172f875012fe205f4', 'dataset_ids': ['1'], 'model_ids': ['2'], 'metrics': ['accuracy'], 'env_vars': [{'key': 'SPLIT_NAME', 'value': 'train'}], 'public': True}\n",
      "{'name': 'TodayExperiment', 'description': 'Exp description', 'publication_ids': [], 'experiment_template_id': '66570433b986f37c731ead6c', 'dataset_ids': ['1'], 'model_ids': ['2'], 'metrics': ['accuracy'], 'env_vars': [{'key': 'SPLIT_NAME', 'value': 'train'}], 'public': True}\n"
     ]
    }
   ],
   "source": [
    "with open('/home/eddie/Projects/exp_TODO_ADD_TEMPLATE_ID.json') as f:\n",
    "    experiment = json.load(f)\n",
    "\n",
    "print(experiment)\n",
    "experiment['experiment_template_id'] = '66570433b986f37c731ead6c'\n",
    "experiment['name'] = 'TodayExperiment'\n",
    "print(experiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create experiment from provided json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentResponse(name='TodayExperiment', description='Exp description', experiment_template_id='66570433b986f37c731ead6c', publication_ids=[], dataset_ids=['1'], model_ids=['2'], env_vars=[EnvironmentVar(key='SPLIT_NAME', value='train')], public=True, id='6657103fb986f37c731ead6d', created_at=datetime.datetime(2024, 5, 29, 11, 23, 42, 208857, tzinfo=TzInfo(UTC)), updated_at=datetime.datetime(2024, 5, 29, 11, 23, 42, 208854, tzinfo=TzInfo(UTC)), archived=False, mine=True)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = my_client.experiments.create_experiment(file=experiment)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display all experiments, we can see our experiment TodayExperiment as the last listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ExperimentResponse(name='Exp01', description='Exp description', experiment_template_id='6655ae4172f875012fe205f4', publication_ids=[], dataset_ids=['1'], model_ids=['2'], env_vars=[EnvironmentVar(key='SPLIT_NAME', value='train')], public=True, id='6655d24f72f875012fe205f9', created_at=datetime.datetime(2024, 5, 28, 12, 47, 10, 518000), updated_at=datetime.datetime(2024, 5, 28, 12, 47, 10, 518000), archived=False, mine=True),\n",
       " ExperimentResponse(name='Exp01', description='Exp description', experiment_template_id='6655ae4172f875012fe205f4', publication_ids=[], dataset_ids=['1'], model_ids=['2'], env_vars=[EnvironmentVar(key='SPLIT_NAME', value='train')], public=True, id='6655d51372f875012fe205fa', created_at=datetime.datetime(2024, 5, 28, 12, 58, 58, 771000), updated_at=datetime.datetime(2024, 5, 28, 13, 4, 36, 529000), archived=True, mine=True),\n",
       " ExperimentResponse(name='TodayExperiment', description='Exp description', experiment_template_id='66570433b986f37c731ead6c', publication_ids=[], dataset_ids=['1'], model_ids=['2'], env_vars=[EnvironmentVar(key='SPLIT_NAME', value='train')], public=True, id='6657103fb986f37c731ead6d', created_at=datetime.datetime(2024, 5, 29, 11, 23, 42, 208000), updated_at=datetime.datetime(2024, 5, 29, 11, 23, 42, 208000), archived=False, mine=True)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = my_client.experiments.get(archived=False)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also list only the number and list only those which are archived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 1\n",
      "Experiment[ExperimentResponse(name='Exp01', description='Exp description', experiment_template_id='6655ae4172f875012fe205f4', publication_ids=[], dataset_ids=['1'], model_ids=['2'], env_vars=[EnvironmentVar(key='SPLIT_NAME', value='train')], public=True, id='6655d51372f875012fe205fa', created_at=datetime.datetime(2024, 5, 28, 12, 58, 58, 771000), updated_at=datetime.datetime(2024, 5, 28, 13, 4, 36, 529000), archived=True, mine=True)]\n"
     ]
    }
   ],
   "source": [
    "resp = my_client.experiments.count(archived=True)\n",
    "print(f'Count: {resp}\\nExperiment{my_client.experiments.get(archived=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the experiment which we will select by providing it's id, lets select the one which we created before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6657103fb986f37c731ead6d']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = my_client.experiments.get(archived=False)\n",
    "experiments = [i.model_dump() for i in resp]\n",
    "id_of_experiment = [i['id'] for i in experiments if i['name'] == 'TodayExperiment']\n",
    "id_of_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentRunResponse(id='6657130bb986f37c731ead6e', created_at=datetime.datetime(2024, 5, 29, 11, 35, 39, 575983, tzinfo=TzInfo(UTC)), updated_at=datetime.datetime(2024, 5, 29, 11, 35, 39, 575995, tzinfo=TzInfo(UTC)), retry_count=0, state=<RunState.CREATED: 'CREATED'>, metrics={}, archived=False, public=True, mine=True, experiment_id='6657103fb986f37c731ead6d')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = my_client.experiments.run_experiment(id=id_of_experiment[0])\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display number of experiment runs and it's count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = my_client.experiments.get_experiments_run_count(id=id_of_experiment[0])\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6657130bb986f37c731ead6e'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = my_client.experiments.get_experiments_run(id=id_of_experiment[0])\n",
    "id_of_run = dict(resp[0])['id']\n",
    "id_of_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display logs from the run which we executed on the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\"workflow_logs\": \"2024-05-29 11:36:00,160 | root | MainThread | INFO | '\n",
      " 'Publishing step:0, cmd: set -a && source .env && set +a && python script.py, '\n",
      " 'total steps 1 to MQ\\\\n2024-05-29 11:40:12,901 | root | MainThread | INFO | '\n",
      " 'Workflow 682f339f-6679-4c0f-a6e5-536786724684 finished. Files available at '\n",
      " '/var/reana/users/00000000-0000-0000-0000-000000000000/workflows/682f339f-6679-4c0f-a6e5-536786724684.\\\\n\\\\n\", '\n",
      " '\"job_logs\": {\"5782b3a9-a2d7-4782-a1e9-1cc77a39af60\": {\"workflow_uuid\": '\n",
      " '\"682f339f-6679-4c0f-a6e5-536786724684\", \"job_name\": \"Execute Python script\", '\n",
      " '\"compute_backend\": \"Kubernetes\", \"backend_job_id\": '\n",
      " '\"reana-run-job-332e98a7-8e4d-4474-96c5-78c9510294e7\", \"docker_img\": '\n",
      " '\"docker.io/marveso/rail-exp-templates:template-66570433b986f37c731ead6c\", '\n",
      " '\"cmd\": \"set -a && source .env && set +a && python script.py\", \"status\": '\n",
      " '\"finished\", \"logs\": \"job: :\\\\n '\n",
      " '/usr/local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: '\n",
      " 'FutureWarning: `resume_download` is deprecated and will be removed in '\n",
      " 'version 1.0.0. Downloads always resume when possible. If you want to force a '\n",
      " 'new download, use `force_download=True`.\\\\n  warnings.warn(\\\\n\\\\rDownloading '\n",
      " 'readme:   0%|          | 0.00/125 [00:00<?, ?B/s]\\\\rDownloading readme: '\n",
      " '100%|\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588| '\n",
      " '125/125 [00:00<00:00, 496kB/s]\\\\n\\\\rDownloading data files:   0%|          | '\n",
      " '0/1 [00:00<?, ?it/s]\\\\n\\\\rDownloading data:   0%|          | 0.00/106 '\n",
      " '[00:00<?, ?B/s]\\\\u001b[A\\\\n\\\\rDownloading data: '\n",
      " '100%|\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588| '\n",
      " '106/106 [00:00<00:00, 349B/s]\\\\u001b[A\\\\rDownloading data: '\n",
      " '100%|\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588| '\n",
      " '106/106 [00:00<00:00, 347B/s]\\\\n\\\\rDownloading data files: '\n",
      " '100%|\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588| '\n",
      " '1/1 [00:00<00:00,  3.25it/s]\\\\rDownloading data files: '\n",
      " '100%|\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588| '\n",
      " '1/1 [00:00<00:00,  3.24it/s]\\\\n\\\\rExtracting data files:   0%|          | '\n",
      " '0/1 [00:00<?, ?it/s]\\\\rExtracting data files: '\n",
      " '100%|\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588| '\n",
      " '1/1 [00:00<00:00, 1332.37it/s]\\\\n\\\\rGenerating train split: 0 examples '\n",
      " '[00:00, ? '\n",
      " 'examples/s]/usr/local/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py:765: '\n",
      " \"FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will \"\n",
      " 'be removed in a future version.\\\\n  return '\n",
      " 'pd.read_csv(xopen(filepath_or_buffer, \\\\\"rb\\\\\", '\n",
      " 'download_config=download_config), **kwargs)\\\\n\\\\rGenerating train split: 5 '\n",
      " 'examples [00:00, 122.67 examples/s]\\\\n\\\\r  0%|          | 0/1 [00:00<?, '\n",
      " '?it/s]Asking to truncate to max_length but no maximum length is provided and '\n",
      " 'the model has no predefined maximum length. Default to no '\n",
      " 'truncation.\\\\n\\\\r100%|\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588| '\n",
      " '1/1 [00:00<00:00, 11.46it/s]\\\\n\\\\n\\\\nCompleted\\\\n\", \"started_at\": '\n",
      " '\"2024-05-29T11:36:00\", \"finished_at\": \"2024-05-29T11:40:07\"}}, '\n",
      " '\"engine_specific\": null}')\n"
     ]
    }
   ],
   "source": [
    "resp = my_client.experiments.logs_experiment_run(id=id_of_run)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also download files from experiment, this call will download script.py into the folder where this code is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_client.experiments.download_experiment_run(id=id_of_run, filepath='script.py', to_dir='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's archive our experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = my_client.experiments.archive(id=id_of_experiment[0], archived=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ExperimentResponse(name='Exp01', description='Exp description', experiment_template_id='6655ae4172f875012fe205f4', publication_ids=[], dataset_ids=['1'], model_ids=['2'], env_vars=[EnvironmentVar(key='SPLIT_NAME', value='train')], public=True, id='6655d51372f875012fe205fa', created_at=datetime.datetime(2024, 5, 28, 12, 58, 58, 771000), updated_at=datetime.datetime(2024, 5, 28, 13, 4, 36, 529000), archived=True, mine=True),\n",
       " ExperimentResponse(name='TodayExperiment', description='Exp description', experiment_template_id='66570433b986f37c731ead6c', publication_ids=[], dataset_ids=['1'], model_ids=['2'], env_vars=[EnvironmentVar(key='SPLIT_NAME', value='train')], public=True, id='6657103fb986f37c731ead6d', created_at=datetime.datetime(2024, 5, 29, 11, 23, 42, 208000), updated_at=datetime.datetime(2024, 5, 29, 11, 58, 37, 332000), archived=True, mine=True)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = my_client.experiments.get(archived=True)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also delete the run which we executed earlier but before that, we have to un-archive the experiment to be able to delete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = my_client.experiments.archive(id=id_of_experiment[0], archived=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_client.experiments.delete_experiment_run(id=id_of_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the run is deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = my_client.experiments.get_experiments_run_count(id=id_of_experiment[0])\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiod-rail-sdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
