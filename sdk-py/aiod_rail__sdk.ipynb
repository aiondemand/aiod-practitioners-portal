{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AIOD_RAIL_API_KEY'] = '9b9c7f0cd6af7656b2b480d842d79863'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import aiod_rail_sdk\n",
    "import aiod_rail_sdk.configuration\n",
    "from client.client import RailClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = '/home/eddie/Projects/script.py'\n",
    "requirements_path = '/home/eddie/Projects/requirements.txt'\n",
    "image_path = '/home/eddie/Projects/Dockerfile'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either load template as whole json and create experiment from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('your-path-to-template-here') as f:\n",
    "  template = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can define our template in code and use paths defined earlier to pass script, requirements and image information to template when calling the method create_experiment_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = {\n",
    "    \"name\": \"MyExperimentNumberTwo\",\n",
    "    \"description\": \"I created this on 30.5.2024\",\n",
    "    \"task\": \"TEXT_CLASSIFICATION\",\n",
    "    \"datasets_schema\": {\n",
    "        \"cardinality\": \"1-1\"\n",
    "    },\n",
    "    \"models_schema\": {\n",
    "        \"cardinality\": \"1-1\"\n",
    "    },\n",
    "    \"envs_required\": [\n",
    "        {\n",
    "            \"name\": \"SPLIT_NAME\",\n",
    "            \"description\": \"name of a subset\"\n",
    "        }\n",
    "    ],\n",
    "    \"envs_optional\": [],\n",
    "    \"available_metrics\": [\n",
    "        \"accuracy\"\n",
    "    ],\n",
    "    \"public\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'APIKeyHeader': '9b9c7f0cd6af7656b2b480d842d79863'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = aiod_rail_sdk.configuration.Configuration(host='http://localhost/api')\n",
    "my_client = RailClient(config)\n",
    "my_client.config.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment Templates endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create experiment template from json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentTemplateResponse(name='MyExperimentNumberTwo', description='I created this on 30.5.2024', task=<TaskType.TEXT_CLASSIFICATION: 'TEXT_CLASSIFICATION'>, datasets_schema=AssetSchema(cardinality=<AssetCardinality.ENUM_1_MINUS_1: '1-1'>), models_schema=AssetSchema(cardinality=<AssetCardinality.ENUM_1_MINUS_1: '1-1'>), envs_required=[EnvironmentVarDef(name='SPLIT_NAME', description='name of a subset')], envs_optional=[], script='import os\\n\\nos.environ[\"HF_HOME\"] = \".\"\\n\\nimport json\\nimport logging\\n\\nimport numpy as np\\nimport sklearn.metrics as m\\nimport torch\\nimport wandb\\nfrom datasets import load_dataset\\nfrom tqdm import tqdm\\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\\n\\n\\ndef get_device():\\n    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n\\n\\ndef wandb_check():\\n    necessary_envs = [\\n        \"WANDB_API_KEY\",\\n        \"WANDB_ENTITY\",\\n        \"WANDB_PROJECT\",\\n        \"WANDB_NAME\",\\n    ]\\n    return set(necessary_envs).issubset(os.environ)\\n\\n\\ndef process_metrics(all_metrics: dict, metrics_filter: list):\\n    metrics_of_interest = {k: all_metrics.get(k) for k in metrics_filter}\\n\\n    os.makedirs(\"./output-temp\", exist_ok=True)\\n    with open(\"./output-temp/metrics.json\", \"w\") as f:\\n        json.dump(metrics_of_interest, f)\\n\\n    if wandb_check():\\n        wandb.init(\\n            project=os.environ[\"WANDB_PROJECT\"],\\n            name=os.environ[\"WANDB_NAME\"],\\n            entity=os.environ[\"WANDB_ENTITY\"],\\n        )\\n        wandb.log(metrics_of_interest)\\n        wandb.finish()\\n\\n\\nif __name__ == \"__main__\":\\n    model_name = os.getenv(\"MODEL_NAMES\").split(\",\")[0]\\n    dataset_name = os.getenv(\"DATASET_NAMES\").split(\",\")[0]\\n    metrics_to_compute = os.getenv(\"METRICS\", default=\"\").split(\",\")\\n    split_name = os.getenv(\"SPLIT_NAME\", default=\"train\")\\n\\n    for attempt in range(5):\\n        try:\\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\\n            model = AutoModelForSequenceClassification.from_pretrained(model_name)\\n            model.to(get_device())\\n            model.eval()\\n\\n            dataset = load_dataset(dataset_name)[split_name]\\n            break\\n        except Exception:\\n            logging.getLogger().warning(\\n                f\"Failed to load model and data (attempt: {attempt})\"\\n            )\\n            continue\\n    else:\\n        logging.getLogger().error(\"Failed to load model and data.\")\\n        exit(1)\\n\\n    batch_size = 8\\n    all_predictions = []\\n    all_labels = []\\n\\n    # predicting\\n    for i in tqdm(range(0, len(dataset), batch_size)):\\n        start_idx = i\\n        end_idx = i + batch_size\\n\\n        sentences = dataset[start_idx:end_idx][\"sentence\"]\\n        labels = dataset[start_idx:end_idx][\"label\"]\\n        labels = torch.tensor(labels)\\n\\n        encoding = tokenizer(\\n            sentences, padding=True, truncation=True, return_tensors=\"pt\"\\n        )\\n        encoding = {k: v.to(get_device()) for k, v in encoding.items()}\\n\\n        with torch.no_grad():\\n            out = model(**encoding)[0]\\n\\n        pred = out.argmax(dim=1)\\n        pred = pred - 1\\n\\n        all_predictions += [pred.cpu().numpy()]\\n        all_labels += [labels.cpu().numpy()]\\n\\n    # calculating metrics\\n    all_predictions = np.hstack(all_predictions)\\n    all_labels = np.hstack(all_labels)\\n\\n    accuracy = m.accuracy_score(all_labels, all_predictions)\\n    precision_macro = m.precision_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n    recall_macro = m.recall_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n    f1_macro = m.f1_score(\\n        all_labels, all_predictions, average=\"macro\", zero_division=1, labels=[-1, 0, 1]\\n    )\\n\\n    metrics = {\\n        \"accuracy\": accuracy,\\n        \"precision_macro\": precision_macro,\\n        \"recall_macro\": recall_macro,\\n        \"f1_macro\": f1_macro,\\n    }\\n    process_metrics(metrics, metrics_to_compute)\\n', pip_requirements='transformers==4.30.2\\ndatasets==2.14.6\\nnumpy==1.25.0\\nscikit-learn==1.2.2\\nwandb==0.15.4\\n--extra-index-url https://download.pytorch.org/whl/cpu\\ntorch==2.0.0+cpu\\n', public=True, id='66584d10824d3eb3dc996093', created_at=datetime.datetime(2024, 5, 30, 9, 55, 28, 161439, tzinfo=TzInfo(UTC)), updated_at=datetime.datetime(2024, 5, 30, 9, 55, 28, 161443, tzinfo=TzInfo(UTC)), state=<TemplateState.CREATED: 'CREATED'>, dockerfile='FROM FROM python:3.9\\nWORKDIR /app\\n\\nCOPY requirements.txt .\\nRUN pip install -r requirements.txt\\nRUN pip install torch==2.0.0+cpu --index-url https://download.pytorch.org/whl/cpu\\n\\nWORKDIR /app\\n\\nCOPY requirements.txt .\\nRUN pip install -r requirements.txt\\n', approved=False, archived=False, mine=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = my_client.experiments_templates.create_experiment_template(template=(script_path, requirements_path, image_path, template))\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'MyExperimentNumberOneUpdated',\n",
       " 'description': 'I updated this experiment on 30.5.2024',\n",
       " 'task': 'TEXT_CLASSIFICATION',\n",
       " 'datasets_schema': {'cardinality': '1-1'},\n",
       " 'models_schema': {'cardinality': '1-1'},\n",
       " 'envs_required': [{'name': 'SPLIT_NAME', 'description': 'name of a subset'}],\n",
       " 'envs_optional': [],\n",
       " 'available_metrics': ['accuracy'],\n",
       " 'public': True}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_changed = template.copy()\n",
    "template_changed['name'] = 'MyExperimentNumberOneUpdated'\n",
    "template_changed['description'] = 'I updated this experiment on 30.5.2024'\n",
    "template_changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the experiment templates, which are not approved and not built and take the one which we created from them to select it for update with template we defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 6655b12972f875012fe205f5\n",
      "name: CreatedExperiment\n",
      "----------------------------------------------------------------------------------------------------\n",
      "id: 6655b24a72f875012fe205f6\n",
      "name: ExperimentChanged\n",
      "----------------------------------------------------------------------------------------------------\n",
      "id: 665846af824d3eb3dc996092\n",
      "name: MyExperimentNumberOne\n",
      "----------------------------------------------------------------------------------------------------\n",
      "id: 66584d10824d3eb3dc996093\n",
      "name: MyExperimentNumberTwo\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "experiment_templates = my_client.experiments_templates.get(finalized=False, approved=False)\n",
    "experiment_template_to_update = None \n",
    "for experiment_template in experiment_templates:\n",
    "    print(f'id: {experiment_template.id}\\nname: {experiment_template.name}')\n",
    "    print(\"-\"*100)\n",
    "    if experiment_template.name == 'MyExperimentNumberOne':\n",
    "        experiment_template_to_update = experiment_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update the selected experiment template, we will check the experiment id to verify that it's the same one we updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 665846af824d3eb3dc996092,\n",
      "name: MyExperimentNumberOneUpdated\n"
     ]
    }
   ],
   "source": [
    "updated_experiment_template = my_client.experiments_templates.update(experiment_template_to_update.id, template=(script_path, requirements_path, image_path, template_changed))\n",
    "print(f'id: {updated_experiment_template.id},\\nname: {updated_experiment_template.name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize experiment templates which are approved and built "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 6655ae4172f875012fe205f4 | name: ExpTempl | approved: True\n",
      "\n",
      "id: 66570433b986f37c731ead6c | name: ExpTempl | approved: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment_templates = my_client.experiments_templates.get(finalized=True, approved=True)\n",
    "for experiment_template in experiment_templates:\n",
    "    print(f\"id: {experiment_template.id} | name: {experiment_template.name} | approved: {experiment_template.approved}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get just their count if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.experiments_templates.count(finalized=True, approved=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the first experiment template by it's id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 6655ae4172f875012fe205f4 | name: ExpTempl | archived: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment_template = my_client.experiments_templates.get_by_id(experiment_templates[0].id)\n",
    "print(f\"id: {experiment_template.id} | name: {experiment_template.name} | archived: {experiment_template.archived}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will archive this experiment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_client.experiments_templates.archive(experiment_template.id, archived=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now verify that experiment template has been archived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 6655ae4172f875012fe205f4 | name: ExpTempl | archived: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment_template = my_client.experiments_templates.get_by_id(experiment_templates[0].id)\n",
    "print(f\"id: {experiment_template.id} | name: {experiment_template.name} | archived: {experiment_template.archived}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view number of datasets we can browse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411987"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.datasets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get two of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset(platform='huggingface', platform_resource_identifier='acronym_identification', name='acronym_identification', date_published=datetime.datetime(2022, 3, 2, 23, 29, 22), same_as='https://huggingface.co/datasets/acronym_identification', is_accessible_for_free=True, version=None, issn=None, measurement_technique=None, temporal_coverage=None, ai_asset_identifier=2, ai_resource_identifier=2, aiod_entry=AIoDEntryRead(editor=[], status='published', date_modified=datetime.datetime(2023, 12, 19, 10, 25, 29), date_created=datetime.datetime(2023, 12, 19, 10, 25, 29)), alternate_name=[], application_area=[], citation=[], contact=[], creator=[], description=Text(plain='Acronym identification training and development sets for the acronym identification task at SDU@AAAI-21.', html=None), distribution=[Distribution(platform=None, platform_resource_identifier=None, checksum=None, checksum_algorithm=None, copyright=None, content_url='https://huggingface.co/datasets/acronym_identification/resolve/refs%2Fconvert%2Fparquet/default/test/0000.parquet', content_size_kb=206100, date_published=None, description='acronym_identification. Config: default. Split: test', encoding_format=None, name='0000.parquet', technology_readiness_level=None), Distribution(platform=None, platform_resource_identifier=None, checksum=None, checksum_algorithm=None, copyright=None, content_url='https://huggingface.co/datasets/acronym_identification/resolve/refs%2Fconvert%2Fparquet/default/train/0000.parquet', content_size_kb=1657991, date_published=None, description='acronym_identification. Config: default. Split: train', encoding_format=None, name='0000.parquet', technology_readiness_level=None), Distribution(platform=None, platform_resource_identifier=None, checksum=None, checksum_algorithm=None, copyright=None, content_url='https://huggingface.co/datasets/acronym_identification/resolve/refs%2Fconvert%2Fparquet/default/validation/0000.parquet', content_size_kb=206916, date_published=None, description='acronym_identification. Config: default. Split: validation', encoding_format=None, name='0000.parquet', technology_readiness_level=None)], funder=[], has_part=[], industrial_sector=[], is_part_of=[], keyword=['source_datasets:original', 'task_categories:token-classification', 'multilinguality:monolingual', 'size_categories:10k<n<100k', 'language:en', 'acronym-identification', 'region:us', 'language_creators:found', 'annotations_creators:expert-generated', 'arxiv:2010.14678', 'license:mit'], license='mit', media=[], note=[], relevant_link=[], relevant_resource=[], relevant_to=[], research_area=[], scientific_domain=[], size=None, spatial_coverage=None, identifier=1),\n",
       " Dataset(platform='huggingface', platform_resource_identifier='ade_corpus_v2', name='ade_corpus_v2', date_published=datetime.datetime(2022, 3, 2, 23, 29, 22), same_as='https://huggingface.co/datasets/ade_corpus_v2', is_accessible_for_free=True, version=None, issn=None, measurement_technique=None, temporal_coverage=None, ai_asset_identifier=3, ai_resource_identifier=3, aiod_entry=AIoDEntryRead(editor=[], status='published', date_modified=datetime.datetime(2023, 12, 19, 10, 25, 29), date_created=datetime.datetime(2023, 12, 19, 10, 25, 29)), alternate_name=[], application_area=[], citation=[], contact=[], creator=[], description=Text(plain=' ADE-Corpus-V2  Dataset: Adverse Drug Reaction Data.\\n This is a dataset for Classification if a sentence is ADE-related (True) or not (False) and Relation Extraction between Adverse Drug Event and Drug.\\n DRUG-AE.rel provides relations between drugs and adverse effects.\\n DRUG-DOSE.rel provides relations between drugs and dosages.\\n ADE-NEG.txt provides all sentences in the ADE corpus that DO NOT contain any drug-related adverse effects.', html=None), distribution=[Distribution(platform=None, platform_resource_identifier=None, checksum=None, checksum_algorithm=None, copyright=None, content_url='https://huggingface.co/datasets/ade_corpus_v2/resolve/refs%2Fconvert%2Fparquet/Ade_corpus_v2_classification/train/0000.parquet', content_size_kb=1706476, date_published=None, description='ade_corpus_v2. Config: Ade_corpus_v2_classification. Split: train', encoding_format=None, name='0000.parquet', technology_readiness_level=None), Distribution(platform=None, platform_resource_identifier=None, checksum=None, checksum_algorithm=None, copyright=None, content_url='https://huggingface.co/datasets/ade_corpus_v2/resolve/refs%2Fconvert%2Fparquet/Ade_corpus_v2_drug_ade_relation/train/0000.parquet', content_size_kb=491362, date_published=None, description='ade_corpus_v2. Config: Ade_corpus_v2_drug_ade_relation. Split: train', encoding_format=None, name='0000.parquet', technology_readiness_level=None), Distribution(platform=None, platform_resource_identifier=None, checksum=None, checksum_algorithm=None, copyright=None, content_url='https://huggingface.co/datasets/ade_corpus_v2/resolve/refs%2Fconvert%2Fparquet/Ade_corpus_v2_drug_dosage_relation/train/0000.parquet', content_size_kb=33004, date_published=None, description='ade_corpus_v2. Config: Ade_corpus_v2_drug_dosage_relation. Split: train', encoding_format=None, name='0000.parquet', technology_readiness_level=None)], funder=[], has_part=[], industrial_sector=[], is_part_of=[], keyword=['source_datasets:original', 'task_categories:token-classification', 'multilinguality:monolingual', 'size_categories:10k<n<100k', 'language:en', 'region:us', 'language_creators:found', 'annotations_creators:expert-generated', 'task_categories:text-classification', 'size_categories:n<1k', 'size_categories:1k<n<10k', 'license:unknown', 'task_ids:fact-checking', 'task_ids:coreference-resolution'], license='unknown', media=[], note=[], relevant_link=[], relevant_resource=[], relevant_to=[], research_area=[], scientific_domain=[], size=None, spatial_coverage=None, identifier=2)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_client.datasets.get(offset=0, limit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either load experiment description from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('your-path-to-experiment-config-json') as f:\n",
    "    experiment = json.load(f)\n",
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can define it by ourselves in code as dictionary, we have to specify an experiment template from which experiment will be created, let's pick the one from approved and built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 6655ae4172f875012fe205f4 | name: ExpTempl | approved: True\n",
      "\n",
      "id: 66570433b986f37c731ead6c | name: ExpTempl | approved: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment_templates = my_client.experiments_templates.get(finalized=True, approved=True)\n",
    "for experiment_template in experiment_templates:\n",
    "    print(f\"id: {experiment_template.id} | name: {experiment_template.name} | approved: {experiment_template.approved}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'MyNewExperiment',\n",
       " 'description': 'I created this experiment on 30.5.2024',\n",
       " 'publication_ids': [],\n",
       " 'experiment_template_id': '66570433b986f37c731ead6c',\n",
       " 'dataset_ids': ['1'],\n",
       " 'model_ids': ['2'],\n",
       " 'env_vars': [{'key': 'SPLIT_NAME', 'value': 'train'}],\n",
       " 'public': True}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "experiment = {\n",
    "    \"name\": \"MyNewExperiment\",\n",
    "    \"description\": \"I created this experiment on 30.5.2024\",\n",
    "    \"publication_ids\": [],\n",
    "    \"experiment_template_id\": experiment_templates[1].id,\n",
    "    \"dataset_ids\": [\n",
    "        \"1\"\n",
    "    ],\n",
    "    \"model_ids\": [\n",
    "        \"2\"\n",
    "    ],\n",
    "    \"env_vars\": [\n",
    "        {\n",
    "            \"key\": \"SPLIT_NAME\",\n",
    "            \"value\": \"train\"\n",
    "        }\n",
    "    ],\n",
    "    \"public\": True\n",
    "}\n",
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create experiment from provided json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentResponse(name='MyNewExperiment', description='I created this experiment on 30.5.2024', experiment_template_id='66570433b986f37c731ead6c', publication_ids=[], dataset_ids=['1'], model_ids=['2'], env_vars=[EnvironmentVar(key='SPLIT_NAME', value='train')], public=True, id='665864354861eda4a581de86', created_at=datetime.datetime(2024, 5, 30, 11, 34, 13, 449350, tzinfo=TzInfo(UTC)), updated_at=datetime.datetime(2024, 5, 30, 11, 34, 13, 449344, tzinfo=TzInfo(UTC)), archived=False, mine=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment = my_client.experiments.create_experiment(experiment=experiment)\n",
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display all experiments, we can see our experiment TodayExperiment as the last listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 6655d24f72f875012fe205f9 | name: Exp01 | archived: False\n",
      "id: 6655d51372f875012fe205fa | name: Exp01 | archived: True\n",
      "id: 6657103fb986f37c731ead6d | name: TodayExperiment | archived: False\n",
      "id: 6658634d4861eda4a581de85 | name: MyNewExperiment | archived: False\n"
     ]
    }
   ],
   "source": [
    "experiments = my_client.experiments.get()\n",
    "for e in experiments:\n",
    "    print(f'id: {e.id} | name: {e.name} | archived: {e.archived}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from output above, one of the experiments is archived, we can use count to display how many of experiments are archived "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = my_client.experiments.count(archived=True)\n",
    "count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the experiment which we will select by providing it's id, lets select the one which we created before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentResponse(name='MyNewExperiment', description='I created this experiment on 30.5.2024', experiment_template_id='66570433b986f37c731ead6c', publication_ids=[], dataset_ids=['1'], model_ids=['2'], env_vars=[EnvironmentVar(key='SPLIT_NAME', value='train')], public=True, id='6658634d4861eda4a581de85', created_at=datetime.datetime(2024, 5, 30, 11, 30, 20, 823000), updated_at=datetime.datetime(2024, 5, 30, 11, 30, 20, 823000), archived=False, mine=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_for_run = [e for e in experiments if e.name == \"MyNewExperiment\"][0]\n",
    "experiment_for_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentRunResponse(id='665865f34861eda4a581de87', created_at=datetime.datetime(2024, 5, 30, 11, 41, 39, 427878, tzinfo=TzInfo(UTC)), updated_at=datetime.datetime(2024, 5, 30, 11, 41, 39, 427882, tzinfo=TzInfo(UTC)), retry_count=0, state=<RunState.CREATED: 'CREATED'>, metrics={}, archived=False, public=True, mine=True, experiment_id='6658634d4861eda4a581de85')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = my_client.experiments.run_experiment(id=experiment_for_run.id)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display number of experiment runs and it's count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running = my_client.experiments.get_experiments_run_count(id=experiment_for_run.id)\n",
    "running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='665865f34861eda4a581de87' created_at=datetime.datetime(2024, 5, 30, 11, 41, 39, 427000) updated_at=datetime.datetime(2024, 5, 30, 11, 42, 54, 625000) retry_count=0 state=<RunState.FINISHED: 'FINISHED'> metrics={} archived=False public=True mine=True experiment_id='6658634d4861eda4a581de85'\n"
     ]
    }
   ],
   "source": [
    "experiment_runs = my_client.experiments.get_experiments_run(id=experiment_for_run.id)\n",
    "for e_r in experiment_runs:\n",
    "    print(f\"{e_r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display logs from the run which we executed on the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\"workflow_logs\": \"2024-05-30 11:41:59,891 | root | MainThread | INFO | '\n",
      " 'Publishing step:0, cmd: set -a && source .env && set +a && python script.py, '\n",
      " 'total steps 1 to MQ\\\\n2024-05-30 11:42:51,039 | root | MainThread | INFO | '\n",
      " 'Workflow e420a9bd-81d8-437d-824f-813328d196df finished. Files available at '\n",
      " '/var/reana/users/00000000-0000-0000-0000-000000000000/workflows/e420a9bd-81d8-437d-824f-813328d196df.\\\\n\\\\n\", '\n",
      " '\"job_logs\": {\"18d8b2cf-8a55-4e05-8f01-66f9677cd620\": {\"workflow_uuid\": '\n",
      " '\"e420a9bd-81d8-437d-824f-813328d196df\", \"job_name\": \"Execute Python script\", '\n",
      " '\"compute_backend\": \"Kubernetes\", \"backend_job_id\": '\n",
      " '\"reana-run-job-588e8a65-4b22-4271-add3-94a118c7107e\", \"docker_img\": '\n",
      " '\"docker.io/marveso/rail-exp-templates:template-66570433b986f37c731ead6c\", '\n",
      " '\"cmd\": \"set -a && source .env && set +a && python script.py\", \"status\": '\n",
      " '\"finished\", \"logs\": \"job: :\\\\n '\n",
      " '/usr/local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: '\n",
      " 'FutureWarning: `resume_download` is deprecated and will be removed in '\n",
      " 'version 1.0.0. Downloads always resume when possible. If you want to force a '\n",
      " 'new download, use `force_download=True`.\\\\n  warnings.warn(\\\\n\\\\rDownloading '\n",
      " 'readme:   0%|          | 0.00/125 [00:00<?, ?B/s]\\\\rDownloading readme: '\n",
      " '100%|\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588| '\n",
      " '125/125 [00:00<00:00, 301kB/s]\\\\n\\\\rDownloading data files:   0%|          | '\n",
      " '0/1 [00:00<?, ?it/s]\\\\n\\\\rDownloading data:   0%|          | 0.00/106 '\n",
      " '[00:00<?, ?B/s]\\\\u001b[A\\\\n\\\\rDownloading data: '\n",
      " '100%|\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588| '\n",
      " '106/106 [00:00<00:00, 348B/s]\\\\u001b[A\\\\rDownloading data: '\n",
      " '100%|\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588| '\n",
      " '106/106 [00:00<00:00, 348B/s]\\\\n\\\\rDownloading data files: '\n",
      " '100%|\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588| '\n",
      " '1/1 [00:00<00:00,  3.25it/s]\\\\rDownloading data files: '\n",
      " '100%|\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588| '\n",
      " '1/1 [00:00<00:00,  3.25it/s]\\\\n\\\\rExtracting data files:   0%|          | '\n",
      " '0/1 [00:00<?, ?it/s]\\\\rExtracting data files: '\n",
      " '100%|\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588| '\n",
      " '1/1 [00:00<00:00, 1283.05it/s]\\\\n\\\\rGenerating train split: 0 examples '\n",
      " '[00:00, ? '\n",
      " 'examples/s]/usr/local/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py:765: '\n",
      " \"FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will \"\n",
      " 'be removed in a future version.\\\\n  return '\n",
      " 'pd.read_csv(xopen(filepath_or_buffer, \\\\\"rb\\\\\", '\n",
      " 'download_config=download_config), **kwargs)\\\\n\\\\rGenerating train split: 5 '\n",
      " 'examples [00:00, 192.71 examples/s]\\\\n\\\\r  0%|          | 0/1 [00:00<?, '\n",
      " '?it/s]Asking to truncate to max_length but no maximum length is provided and '\n",
      " 'the model has no predefined maximum length. Default to no '\n",
      " 'truncation.\\\\n\\\\r100%|\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588| '\n",
      " '1/1 [00:00<00:00, 12.08it/s]\\\\n\\\\n\\\\nCompleted\\\\n\", \"started_at\": '\n",
      " '\"2024-05-30T11:41:59\", \"finished_at\": \"2024-05-30T11:42:47\"}}, '\n",
      " '\"engine_specific\": null}')\n"
     ]
    }
   ],
   "source": [
    "logs = my_client.experiments.logs_experiment_run(id=experiment_runs[0].id)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also download files from experiment, this call will download script.py into the folder where this code is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_client.experiments.download_experiment_run(id=experiment_runs[0].id, filepath='script.py', to_dir='run_out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's archive our experiment which we ran earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_client.experiments.archive(id=experiment_for_run.id, archived=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from output, our experiment MyNewExperiment has been archived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 6655d51372f875012fe205fa | name: Exp01 | archived: True\n",
      "id: 6658634d4861eda4a581de85 | name: MyNewExperiment | archived: True\n"
     ]
    }
   ],
   "source": [
    "archived_experiments = my_client.experiments.get(archived=True)\n",
    "\n",
    "for a_e in archived_experiments:\n",
    "    print(f\"id: {a_e.id} | name: {a_e.name} | archived: {a_e.archived}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also delete the run which we executed earlier but before that, we have to un-archive the experiment to be able to delete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "unarchived_experiment = my_client.experiments.archive(id=experiment_for_run.id, archived=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_client.experiments.delete_experiment_run(id=experiment_runs[0].id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the run is deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = my_client.experiments.get_experiments_run_count(id=experiment_for_run.id)\n",
    "count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiod-rail-sdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
