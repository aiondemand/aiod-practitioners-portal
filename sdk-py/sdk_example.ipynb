{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:53:56.443417Z",
     "start_time": "2024-05-31T09:53:55.619096Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from aiod_rail_sdk import Configuration\n",
    "from aiod_rail_sdk.clients import RailClient\n",
    "\n",
    "os.environ[\"AIOD_RAIL_API_KEY\"] = \"your-api-key\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Create RAIL client"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:53:57.630117Z",
     "start_time": "2024-05-31T09:53:57.626545Z"
    }
   },
   "source": [
    "config = Configuration(host=\"http://localhost/api\")\n",
    "my_client = RailClient(config)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Experiment Templates endpoints"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare template specification"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can either load template specification as whole json and create ExperimentTemplate from it"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open('your-path-to-template-spec-here.json') as f:\n",
    "  template = json.load(f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Or we can define our template in code and use paths to local files created earlier (script, requirements and Dockerfile)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:54:03.054211Z",
     "start_time": "2024-05-31T09:54:03.050538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "script_path = \"../backend/dev-scripts/experiments/script.py\"\n",
    "requirements_path = \"../backend/dev-scripts/experiments/requirements.txt\"\n",
    "base_image = \"python:3.9\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:54:03.490052Z",
     "start_time": "2024-05-31T09:54:03.485920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "template_config = {\n",
    "    \"name\": \"MyExperimentTemplate\",\n",
    "    \"description\": \"I created this on 30.5.2024\",\n",
    "    \"task\": \"TEXT_CLASSIFICATION\",\n",
    "    \"datasets_schema\": {\n",
    "        \"cardinality\": \"1-1\"\n",
    "    },\n",
    "    \"models_schema\": {\n",
    "        \"cardinality\": \"1-1\"\n",
    "    },\n",
    "    \"envs_required\": [\n",
    "        {\n",
    "            \"name\": \"SPLIT_NAME\",\n",
    "            \"description\": \"name of a subset\"\n",
    "        }\n",
    "    ],\n",
    "    \"envs_optional\": [],\n",
    "    \"available_metrics\": [\n",
    "        \"accuracy\"\n",
    "    ],\n",
    "    \"public\": True\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Create experiment template"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:54:07.072485Z",
     "start_time": "2024-05-31T09:54:07.051315Z"
    }
   },
   "source": [
    "resp = my_client.experiments_templates.create_experiment_template(\n",
    "    template=(script_path, requirements_path, base_image, template_config)\n",
    ")\n",
    "pprint(resp.to_dict(), sort_dicts=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'MyExperimentTemplate',\n",
      " 'description': 'I created this on 30.5.2024',\n",
      " 'task': <TaskType.TEXT_CLASSIFICATION: 'TEXT_CLASSIFICATION'>,\n",
      " 'datasets_schema': {'cardinality': <AssetCardinality.ENUM_1_MINUS_1: '1-1'>},\n",
      " 'models_schema': {'cardinality': <AssetCardinality.ENUM_1_MINUS_1: '1-1'>},\n",
      " 'envs_required': [{'name': 'SPLIT_NAME', 'description': 'name of a subset'}],\n",
      " 'envs_optional': [],\n",
      " 'script': 'import os\\n'\n",
      "           '\\n'\n",
      "           'os.environ[\"HF_HOME\"] = \".\"\\n'\n",
      "           '\\n'\n",
      "           'import json\\n'\n",
      "           'import logging\\n'\n",
      "           '\\n'\n",
      "           'import numpy as np\\n'\n",
      "           'import sklearn.metrics as m\\n'\n",
      "           'import torch\\n'\n",
      "           'import wandb\\n'\n",
      "           'from datasets import load_dataset\\n'\n",
      "           'from tqdm import tqdm\\n'\n",
      "           'from transformers import AutoModelForSequenceClassification, '\n",
      "           'AutoTokenizer\\n'\n",
      "           '\\n'\n",
      "           '\\n'\n",
      "           'def get_device():\\n'\n",
      "           '    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n'\n",
      "           '\\n'\n",
      "           '\\n'\n",
      "           'def wandb_check():\\n'\n",
      "           '    necessary_envs = [\\n'\n",
      "           '        \"WANDB_API_KEY\",\\n'\n",
      "           '        \"WANDB_ENTITY\",\\n'\n",
      "           '        \"WANDB_PROJECT\",\\n'\n",
      "           '        \"WANDB_NAME\",\\n'\n",
      "           '    ]\\n'\n",
      "           '    return set(necessary_envs).issubset(os.environ)\\n'\n",
      "           '\\n'\n",
      "           '\\n'\n",
      "           'def save_metrics(metrics: dict):\\n'\n",
      "           '    os.makedirs(\"./output-temp\", exist_ok=True)\\n'\n",
      "           '    with open(\"./output-temp/metrics.json\", \"w\") as f:\\n'\n",
      "           '        json.dump(metrics, f)\\n'\n",
      "           '\\n'\n",
      "           '    if wandb_check():\\n'\n",
      "           '        wandb.init(\\n'\n",
      "           '            project=os.environ[\"WANDB_PROJECT\"],\\n'\n",
      "           '            name=os.environ[\"WANDB_NAME\"],\\n'\n",
      "           '            entity=os.environ[\"WANDB_ENTITY\"],\\n'\n",
      "           '        )\\n'\n",
      "           '        wandb.log(metrics)\\n'\n",
      "           '        wandb.finish()\\n'\n",
      "           '\\n'\n",
      "           '\\n'\n",
      "           'if __name__ == \"__main__\":\\n'\n",
      "           '    model_name = os.getenv(\"MODEL_NAMES\").split(\",\")[0]\\n'\n",
      "           '    dataset_name = os.getenv(\"DATASET_NAMES\").split(\",\")[0]\\n'\n",
      "           '    model_id = os.getenv(\"MODEL_IDS\").split(\",\")[0]\\n'\n",
      "           '    dataset_id = os.getenv(\"DATASET_IDS\").split(\",\")[0]\\n'\n",
      "           '    split_name = os.getenv(\"SPLIT_NAME\", default=\"train\")\\n'\n",
      "           '\\n'\n",
      "           '    for attempt in range(5):\\n'\n",
      "           '        try:\\n'\n",
      "           '            tokenizer = AutoTokenizer.from_pretrained(model_name)\\n'\n",
      "           '            model = '\n",
      "           'AutoModelForSequenceClassification.from_pretrained(model_name)\\n'\n",
      "           '            model.to(get_device())\\n'\n",
      "           '            model.eval()\\n'\n",
      "           '\\n'\n",
      "           '            dataset = load_dataset(dataset_name)[split_name]\\n'\n",
      "           '            break\\n'\n",
      "           '        except Exception:\\n'\n",
      "           '            logging.getLogger().warning(\\n'\n",
      "           '                f\"Failed to load model and data (attempt: '\n",
      "           '{attempt})\"\\n'\n",
      "           '            )\\n'\n",
      "           '            continue\\n'\n",
      "           '    else:\\n'\n",
      "           '        logging.getLogger().error(\"Failed to load model and '\n",
      "           'data.\")\\n'\n",
      "           '        exit(1)\\n'\n",
      "           '\\n'\n",
      "           '    batch_size = 8\\n'\n",
      "           '    all_predictions = []\\n'\n",
      "           '    all_labels = []\\n'\n",
      "           '\\n'\n",
      "           '    # predicting\\n'\n",
      "           '    for i in tqdm(range(0, len(dataset), batch_size)):\\n'\n",
      "           '        start_idx = i\\n'\n",
      "           '        end_idx = i + batch_size\\n'\n",
      "           '\\n'\n",
      "           '        sentences = dataset[start_idx:end_idx][\"sentence\"]\\n'\n",
      "           '        labels = dataset[start_idx:end_idx][\"label\"]\\n'\n",
      "           '        labels = torch.tensor(labels)\\n'\n",
      "           '\\n'\n",
      "           '        encoding = tokenizer(\\n'\n",
      "           '            sentences, padding=True, truncation=True, '\n",
      "           'return_tensors=\"pt\"\\n'\n",
      "           '        )\\n'\n",
      "           '        encoding = {k: v.to(get_device()) for k, v in '\n",
      "           'encoding.items()}\\n'\n",
      "           '\\n'\n",
      "           '        with torch.no_grad():\\n'\n",
      "           '            out = model(**encoding)[0]\\n'\n",
      "           '\\n'\n",
      "           '        pred = out.argmax(dim=1)\\n'\n",
      "           '        pred = pred - 1\\n'\n",
      "           '\\n'\n",
      "           '        all_predictions += [pred.cpu().numpy()]\\n'\n",
      "           '        all_labels += [labels.cpu().numpy()]\\n'\n",
      "           '\\n'\n",
      "           '    # calculating metrics\\n'\n",
      "           '    all_predictions = np.hstack(all_predictions)\\n'\n",
      "           '    all_labels = np.hstack(all_labels)\\n'\n",
      "           '\\n'\n",
      "           '    accuracy = m.accuracy_score(all_labels, all_predictions)\\n'\n",
      "           '    precision_macro = m.precision_score(\\n'\n",
      "           '        all_labels, all_predictions, average=\"macro\", '\n",
      "           'zero_division=1, labels=[-1, 0, 1]\\n'\n",
      "           '    )\\n'\n",
      "           '    recall_macro = m.recall_score(\\n'\n",
      "           '        all_labels, all_predictions, average=\"macro\", '\n",
      "           'zero_division=1, labels=[-1, 0, 1]\\n'\n",
      "           '    )\\n'\n",
      "           '    f1_macro = m.f1_score(\\n'\n",
      "           '        all_labels, all_predictions, average=\"macro\", '\n",
      "           'zero_division=1, labels=[-1, 0, 1]\\n'\n",
      "           '    )\\n'\n",
      "           '\\n'\n",
      "           '    metrics = {\\n'\n",
      "           '        \"accuracy\": accuracy,\\n'\n",
      "           '        \"precision_macro\": precision_macro,\\n'\n",
      "           '        \"recall_macro\": recall_macro,\\n'\n",
      "           '        \"f1_macro\": f1_macro,\\n'\n",
      "           '    }\\n'\n",
      "           '    save_metrics(metrics)\\n',\n",
      " 'pip_requirements': 'transformers==4.30.2\\n'\n",
      "                     'datasets==2.14.6\\n'\n",
      "                     'numpy==1.25.0\\n'\n",
      "                     'scikit-learn==1.2.2\\n'\n",
      "                     'wandb==0.15.4\\n'\n",
      "                     '--extra-index-url https://download.pytorch.org/whl/cpu\\n'\n",
      "                     'torch==2.0.0+cpu\\n',\n",
      " 'public': True,\n",
      " 'id': '66599e3fd42a40985a8d26a5',\n",
      " 'created_at': datetime.datetime(2024, 5, 31, 9, 54, 7, 59218, tzinfo=TzInfo(UTC)),\n",
      " 'updated_at': datetime.datetime(2024, 5, 31, 9, 54, 7, 59222, tzinfo=TzInfo(UTC)),\n",
      " 'state': <TemplateState.CREATED: 'CREATED'>,\n",
      " 'dockerfile': 'FROM python:3.9\\n'\n",
      "               'WORKDIR /app\\n'\n",
      "               '\\n'\n",
      "               'COPY requirements.txt .\\n'\n",
      "               'RUN pip install -r requirements.txt\\n',\n",
      " 'approved': False,\n",
      " 'archived': False,\n",
      " 'mine': True}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Update experiment template"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:54:19.693305Z",
     "start_time": "2024-05-31T09:54:19.686621Z"
    }
   },
   "source": [
    "template_config_updated = template_config\n",
    "template_config_updated[\"name\"] = 'MyExperimentTemplateUpdated'\n",
    "template_config_updated[\"description\"] = 'I updated this experiment on 30.5.2024'\n",
    "pprint(template_config_updated)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'available_metrics': ['accuracy'],\n",
      " 'base_image': 'python:3.9',\n",
      " 'datasets_schema': {'cardinality': '1-1'},\n",
      " 'description': 'I updated this experiment on 30.5.2024',\n",
      " 'envs_optional': [],\n",
      " 'envs_required': [{'description': 'name of a subset', 'name': 'SPLIT_NAME'}],\n",
      " 'models_schema': {'cardinality': '1-1'},\n",
      " 'name': 'MyExperimentTemplateUpdated',\n",
      " 'pip_requirements': 'transformers==4.30.2\\n'\n",
      "                     'datasets==2.14.6\\n'\n",
      "                     'numpy==1.25.0\\n'\n",
      "                     'scikit-learn==1.2.2\\n'\n",
      "                     'wandb==0.15.4\\n'\n",
      "                     '--extra-index-url https://download.pytorch.org/whl/cpu\\n'\n",
      "                     'torch==2.0.0+cpu\\n',\n",
      " 'public': True,\n",
      " 'script': 'import os\\n'\n",
      "           '\\n'\n",
      "           'os.environ[\"HF_HOME\"] = \".\"\\n'\n",
      "           '\\n'\n",
      "           'import json\\n'\n",
      "           'import logging\\n'\n",
      "           '\\n'\n",
      "           'import numpy as np\\n'\n",
      "           'import sklearn.metrics as m\\n'\n",
      "           'import torch\\n'\n",
      "           'import wandb\\n'\n",
      "           'from datasets import load_dataset\\n'\n",
      "           'from tqdm import tqdm\\n'\n",
      "           'from transformers import AutoModelForSequenceClassification, '\n",
      "           'AutoTokenizer\\n'\n",
      "           '\\n'\n",
      "           '\\n'\n",
      "           'def get_device():\\n'\n",
      "           '    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\\n'\n",
      "           '\\n'\n",
      "           '\\n'\n",
      "           'def wandb_check():\\n'\n",
      "           '    necessary_envs = [\\n'\n",
      "           '        \"WANDB_API_KEY\",\\n'\n",
      "           '        \"WANDB_ENTITY\",\\n'\n",
      "           '        \"WANDB_PROJECT\",\\n'\n",
      "           '        \"WANDB_NAME\",\\n'\n",
      "           '    ]\\n'\n",
      "           '    return set(necessary_envs).issubset(os.environ)\\n'\n",
      "           '\\n'\n",
      "           '\\n'\n",
      "           'def save_metrics(metrics: dict):\\n'\n",
      "           '    os.makedirs(\"./output-temp\", exist_ok=True)\\n'\n",
      "           '    with open(\"./output-temp/metrics.json\", \"w\") as f:\\n'\n",
      "           '        json.dump(metrics, f)\\n'\n",
      "           '\\n'\n",
      "           '    if wandb_check():\\n'\n",
      "           '        wandb.init(\\n'\n",
      "           '            project=os.environ[\"WANDB_PROJECT\"],\\n'\n",
      "           '            name=os.environ[\"WANDB_NAME\"],\\n'\n",
      "           '            entity=os.environ[\"WANDB_ENTITY\"],\\n'\n",
      "           '        )\\n'\n",
      "           '        wandb.log(metrics)\\n'\n",
      "           '        wandb.finish()\\n'\n",
      "           '\\n'\n",
      "           '\\n'\n",
      "           'if __name__ == \"__main__\":\\n'\n",
      "           '    model_name = os.getenv(\"MODEL_NAMES\").split(\",\")[0]\\n'\n",
      "           '    dataset_name = os.getenv(\"DATASET_NAMES\").split(\",\")[0]\\n'\n",
      "           '    model_id = os.getenv(\"MODEL_IDS\").split(\",\")[0]\\n'\n",
      "           '    dataset_id = os.getenv(\"DATASET_IDS\").split(\",\")[0]\\n'\n",
      "           '    split_name = os.getenv(\"SPLIT_NAME\", default=\"train\")\\n'\n",
      "           '\\n'\n",
      "           '    for attempt in range(5):\\n'\n",
      "           '        try:\\n'\n",
      "           '            tokenizer = AutoTokenizer.from_pretrained(model_name)\\n'\n",
      "           '            model = '\n",
      "           'AutoModelForSequenceClassification.from_pretrained(model_name)\\n'\n",
      "           '            model.to(get_device())\\n'\n",
      "           '            model.eval()\\n'\n",
      "           '\\n'\n",
      "           '            dataset = load_dataset(dataset_name)[split_name]\\n'\n",
      "           '            break\\n'\n",
      "           '        except Exception:\\n'\n",
      "           '            logging.getLogger().warning(\\n'\n",
      "           '                f\"Failed to load model and data (attempt: '\n",
      "           '{attempt})\"\\n'\n",
      "           '            )\\n'\n",
      "           '            continue\\n'\n",
      "           '    else:\\n'\n",
      "           '        logging.getLogger().error(\"Failed to load model and '\n",
      "           'data.\")\\n'\n",
      "           '        exit(1)\\n'\n",
      "           '\\n'\n",
      "           '    batch_size = 8\\n'\n",
      "           '    all_predictions = []\\n'\n",
      "           '    all_labels = []\\n'\n",
      "           '\\n'\n",
      "           '    # predicting\\n'\n",
      "           '    for i in tqdm(range(0, len(dataset), batch_size)):\\n'\n",
      "           '        start_idx = i\\n'\n",
      "           '        end_idx = i + batch_size\\n'\n",
      "           '\\n'\n",
      "           '        sentences = dataset[start_idx:end_idx][\"sentence\"]\\n'\n",
      "           '        labels = dataset[start_idx:end_idx][\"label\"]\\n'\n",
      "           '        labels = torch.tensor(labels)\\n'\n",
      "           '\\n'\n",
      "           '        encoding = tokenizer(\\n'\n",
      "           '            sentences, padding=True, truncation=True, '\n",
      "           'return_tensors=\"pt\"\\n'\n",
      "           '        )\\n'\n",
      "           '        encoding = {k: v.to(get_device()) for k, v in '\n",
      "           'encoding.items()}\\n'\n",
      "           '\\n'\n",
      "           '        with torch.no_grad():\\n'\n",
      "           '            out = model(**encoding)[0]\\n'\n",
      "           '\\n'\n",
      "           '        pred = out.argmax(dim=1)\\n'\n",
      "           '        pred = pred - 1\\n'\n",
      "           '\\n'\n",
      "           '        all_predictions += [pred.cpu().numpy()]\\n'\n",
      "           '        all_labels += [labels.cpu().numpy()]\\n'\n",
      "           '\\n'\n",
      "           '    # calculating metrics\\n'\n",
      "           '    all_predictions = np.hstack(all_predictions)\\n'\n",
      "           '    all_labels = np.hstack(all_labels)\\n'\n",
      "           '\\n'\n",
      "           '    accuracy = m.accuracy_score(all_labels, all_predictions)\\n'\n",
      "           '    precision_macro = m.precision_score(\\n'\n",
      "           '        all_labels, all_predictions, average=\"macro\", '\n",
      "           'zero_division=1, labels=[-1, 0, 1]\\n'\n",
      "           '    )\\n'\n",
      "           '    recall_macro = m.recall_score(\\n'\n",
      "           '        all_labels, all_predictions, average=\"macro\", '\n",
      "           'zero_division=1, labels=[-1, 0, 1]\\n'\n",
      "           '    )\\n'\n",
      "           '    f1_macro = m.f1_score(\\n'\n",
      "           '        all_labels, all_predictions, average=\"macro\", '\n",
      "           'zero_division=1, labels=[-1, 0, 1]\\n'\n",
      "           '    )\\n'\n",
      "           '\\n'\n",
      "           '    metrics = {\\n'\n",
      "           '        \"accuracy\": accuracy,\\n'\n",
      "           '        \"precision_macro\": precision_macro,\\n'\n",
      "           '        \"recall_macro\": recall_macro,\\n'\n",
      "           '        \"f1_macro\": f1_macro,\\n'\n",
      "           '    }\\n'\n",
      "           '    save_metrics(metrics)\\n',\n",
      " 'task': 'TEXT_CLASSIFICATION'}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Let's check the experiment templates, which are not approved yet and take the one we just created to select it for update with template config we defined above"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:54:56.011010Z",
     "start_time": "2024-05-31T09:54:55.996738Z"
    }
   },
   "source": [
    "experiment_templates = my_client.experiments_templates.get(approved=False)\n",
    "experiment_template_to_update = None \n",
    "\n",
    "for experiment_template in experiment_templates:\n",
    "    print(f\"id: {experiment_template.id}\\nname: {experiment_template.name}\")\n",
    "    print(\"-\"*100)\n",
    "    if experiment_template.name == \"MyExperimentTemplate\":\n",
    "        experiment_template_to_update = experiment_template"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 66599e3fd42a40985a8d26a5\n",
      "name: MyExperimentTemplate\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update the selected experiment template, we will check the experiment id to verify that it's the same one we updated"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:55:06.269943Z",
     "start_time": "2024-05-31T09:55:06.251436Z"
    }
   },
   "source": [
    "updated_experiment_template = my_client.experiments_templates.update(\n",
    "    experiment_template_to_update.id,\n",
    "    template=(script_path, requirements_path, base_image, template_config_updated)\n",
    ")\n",
    "print(f'id: {updated_experiment_template.id},\\nname: {updated_experiment_template.name}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 66599e3fd42a40985a8d26a5,\n",
      "name: MyExperimentTemplateUpdated\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize experiment templates which are approved and built "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:58:18.866145Z",
     "start_time": "2024-05-31T09:58:18.843037Z"
    }
   },
   "source": [
    "experiment_templates = my_client.experiments_templates.get(finalized=True, approved=True)\n",
    "for experiment_template in experiment_templates:\n",
    "    print(f\"id: {experiment_template.id} | name: {experiment_template.name} | approved: {experiment_template.approved}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 665998b7d42a40985a8d26a4 | name: ExampleTemplate | approved: True\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get just their count if needed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:58:22.629417Z",
     "start_time": "2024-05-31T09:58:22.608334Z"
    }
   },
   "source": [
    "my_client.experiments_templates.count(finalized=True, approved=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the first experiment template by it's id"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:58:28.611418Z",
     "start_time": "2024-05-31T09:58:28.594614Z"
    }
   },
   "source": [
    "experiment_template = my_client.experiments_templates.get_by_id(experiment_templates[0].id)\n",
    "print(f\"id: {experiment_template.id} | name: {experiment_template.name} | archived: {experiment_template.archived}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 665998b7d42a40985a8d26a4 | name: ExampleTemplate | archived: False\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will archive this experiment template"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:58:43.034065Z",
     "start_time": "2024-05-31T09:58:43.018389Z"
    }
   },
   "source": [
    "my_client.experiments_templates.archive(experiment_template.id, archived=True)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now verify that experiment template has been archived"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:58:45.093646Z",
     "start_time": "2024-05-31T09:58:45.075934Z"
    }
   },
   "source": [
    "experiment_template = my_client.experiments_templates.get_by_id(experiment_templates[0].id)\n",
    "print(f\"id: {experiment_template.id} | name: {experiment_template.name} | archived: {experiment_template.archived}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 665998b7d42a40985a8d26a4 | name: ExampleTemplate | archived: True\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Datasets endpoints"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view number of datasets we can browse"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:58:53.916601Z",
     "start_time": "2024-05-31T09:58:53.334756Z"
    }
   },
   "source": [
    "my_client.datasets.count()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411987"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get two of them"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T09:58:58.950961Z",
     "start_time": "2024-05-31T09:58:58.775330Z"
    }
   },
   "source": [
    "example_datasets = my_client.datasets.get(offset=0, limit=2)\n",
    "pprint(list(map(lambda x: (x.name, x.description), example_datasets)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('acronym_identification',\n",
      "  Text(plain='Acronym identification training and development sets for the acronym identification task at SDU@AAAI-21.', html=None)),\n",
      " ('ade_corpus_v2',\n",
      "  Text(plain=' ADE-Corpus-V2  Dataset: Adverse Drug Reaction Data.\\n This is a dataset for Classification if a sentence is ADE-related (True) or not (False) and Relation Extraction between Adverse Drug Event and Drug.\\n DRUG-AE.rel provides relations between drugs and adverse effects.\\n DRUG-DOSE.rel provides relations between drugs and dosages.\\n ADE-NEG.txt provides all sentences in the ADE corpus that DO NOT contain any drug-related adverse effects.', html=None))]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Experiments endpoints"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either load experiment description from file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open('your-path-to-experiment-config-json') as f:\n",
    "    experiment = json.load(f)\n",
    "experiment"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Or we can define it by ourselves in code as dictionary. We have to specify an experiment template from which experiment will be created, let's pick the one that is ready to be use"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T10:01:05.581935Z",
     "start_time": "2024-05-31T10:01:05.564174Z"
    }
   },
   "source": [
    "experiment_templates = my_client.experiments_templates.get(finalized=True, approved=True)\n",
    "\n",
    "for experiment_template in experiment_templates:\n",
    "    print(f\"id: {experiment_template.id} | name: {experiment_template.name} | approved: {experiment_template.approved}\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 665998b7d42a40985a8d26a4 | name: ExampleTemplate | approved: True\n",
      "\n",
      "id: 66599e3fd42a40985a8d26a5 | name: MyExperimentTemplateUpdated | approved: True\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T10:01:57.817477Z",
     "start_time": "2024-05-31T10:01:57.811738Z"
    }
   },
   "source": [
    "experiment = {\n",
    "    \"name\": \"MyNewExperiment\",\n",
    "    \"description\": \"I created this experiment on 31.5.2024\",\n",
    "    \"publication_ids\": [],\n",
    "    \"experiment_template_id\": experiment_templates[1].id,\n",
    "    \"dataset_ids\": [\n",
    "        \"1\"\n",
    "    ],\n",
    "    \"model_ids\": [\n",
    "        \"2\"\n",
    "    ],\n",
    "    \"env_vars\": [\n",
    "        {\n",
    "            \"key\": \"SPLIT_NAME\",\n",
    "            \"value\": \"train\"\n",
    "        }\n",
    "    ],\n",
    "    \"public\": True\n",
    "}\n",
    "experiment"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'MyNewExperiment',\n",
       " 'description': 'I created this experiment on 31.5.2024',\n",
       " 'publication_ids': [],\n",
       " 'experiment_template_id': '66599e3fd42a40985a8d26a5',\n",
       " 'dataset_ids': ['1'],\n",
       " 'model_ids': ['2'],\n",
       " 'env_vars': [{'key': 'SPLIT_NAME', 'value': 'train'}],\n",
       " 'public': True}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create experiment from provided json file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T10:02:11.321765Z",
     "start_time": "2024-05-31T10:02:10.887616Z"
    }
   },
   "source": [
    "experiment = my_client.experiments.create_experiment(experiment=experiment)\n",
    "experiment.to_dict()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'MyNewExperiment',\n",
       " 'description': 'I created this experiment on 31.5.2024',\n",
       " 'experiment_template_id': '66599e3fd42a40985a8d26a5',\n",
       " 'publication_ids': [],\n",
       " 'dataset_ids': ['1'],\n",
       " 'model_ids': ['2'],\n",
       " 'env_vars': [{'key': 'SPLIT_NAME', 'value': 'train'}],\n",
       " 'public': True,\n",
       " 'id': '6659a023d42a40985a8d26a6',\n",
       " 'created_at': datetime.datetime(2024, 5, 31, 10, 2, 10, 896985, tzinfo=TzInfo(UTC)),\n",
       " 'updated_at': datetime.datetime(2024, 5, 31, 10, 2, 10, 896980, tzinfo=TzInfo(UTC)),\n",
       " 'archived': False,\n",
       " 'mine': True}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Let's display all experiments. We can see our new experiment there"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T10:07:11.388957Z",
     "start_time": "2024-05-31T10:07:11.374960Z"
    }
   },
   "source": [
    "experiments = my_client.experiments.get()\n",
    "for e in experiments:\n",
    "    print(f'id: {e.id} | name: {e.name} | archived: {e.archived}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 6659a023d42a40985a8d26a6 | name: MyNewExperiment | archived: False\n",
      "id: 6659a09cd42a40985a8d26a7 | name: SimpleExperiment | archived: True\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from output above, one of the experiments is archived, we can use count to display how many of experiments are archived "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T10:07:20.661480Z",
     "start_time": "2024-05-31T10:07:20.649241Z"
    }
   },
   "source": "my_client.experiments.count(archived=True)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run experiment"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We can now run the experiment which we will select by providing its id or name. Let's select the one which we created before and run it."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T10:28:07.699986Z",
     "start_time": "2024-05-31T10:28:07.681750Z"
    }
   },
   "source": [
    "experiment_for_run = my_client.experiments.get(query=\"SimpleExperiment\")[0]\n",
    "experiment_for_run.to_dict()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'SimpleExperiment',\n",
       " 'description': 'SimpleExperiment',\n",
       " 'experiment_template_id': '665998b7d42a40985a8d26a4',\n",
       " 'publication_ids': [],\n",
       " 'dataset_ids': ['1'],\n",
       " 'model_ids': ['2'],\n",
       " 'env_vars': [],\n",
       " 'public': False,\n",
       " 'id': '6659a09cd42a40985a8d26a7',\n",
       " 'created_at': datetime.datetime(2024, 5, 31, 10, 4, 12, 462000),\n",
       " 'updated_at': datetime.datetime(2024, 5, 31, 10, 26, 27, 971000),\n",
       " 'archived': False,\n",
       " 'mine': True}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T10:28:13.626570Z",
     "start_time": "2024-05-31T10:28:13.485369Z"
    }
   },
   "source": [
    "run = my_client.experiments.run_experiment(id=experiment_for_run.id)\n",
    "run.to_dict()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '6659a63dedac6aaf345fd85d',\n",
       " 'created_at': datetime.datetime(2024, 5, 31, 10, 28, 13, 618646, tzinfo=TzInfo(UTC)),\n",
       " 'updated_at': datetime.datetime(2024, 5, 31, 10, 28, 13, 618654, tzinfo=TzInfo(UTC)),\n",
       " 'retry_count': 0,\n",
       " 'state': <RunState.CREATED: 'CREATED'>,\n",
       " 'metrics': {},\n",
       " 'archived': False,\n",
       " 'public': False,\n",
       " 'mine': True,\n",
       " 'experiment_id': '6659a09cd42a40985a8d26a7'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display number of experiment runs and it's count"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T10:28:20.298784Z",
     "start_time": "2024-05-31T10:28:20.283994Z"
    }
   },
   "source": "my_client.experiments.get_experiment_runs_count(id=experiment_for_run.id)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T10:28:23.692543Z",
     "start_time": "2024-05-31T10:28:23.674388Z"
    }
   },
   "source": [
    "experiment_runs = my_client.experiments.get_experiment_runs(id=experiment_for_run.id)\n",
    "for exp_run in experiment_runs:\n",
    "    display(exp_run.to_dict())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '6659a5d5edac6aaf345fd85c',\n",
       " 'created_at': datetime.datetime(2024, 5, 31, 10, 26, 29, 694000),\n",
       " 'updated_at': datetime.datetime(2024, 5, 31, 10, 27, 4, 5000),\n",
       " 'retry_count': 0,\n",
       " 'state': <RunState.FINISHED: 'FINISHED'>,\n",
       " 'metrics': {},\n",
       " 'archived': False,\n",
       " 'public': False,\n",
       " 'mine': True,\n",
       " 'experiment_id': '6659a09cd42a40985a8d26a7'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': '6659a63dedac6aaf345fd85d',\n",
       " 'created_at': datetime.datetime(2024, 5, 31, 10, 28, 13, 618000),\n",
       " 'updated_at': datetime.datetime(2024, 5, 31, 10, 28, 19, 44000),\n",
       " 'retry_count': 0,\n",
       " 'state': <RunState.IN_PROGRESS: 'IN_PROGRESS'>,\n",
       " 'metrics': {},\n",
       " 'archived': False,\n",
       " 'public': False,\n",
       " 'mine': True,\n",
       " 'experiment_id': '6659a09cd42a40985a8d26a7'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display logs from the run which we executed on the experiment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T10:29:20.353815Z",
     "start_time": "2024-05-31T10:29:20.340949Z"
    }
   },
   "source": [
    "logs = my_client.experiments.logs_experiment_run(id=experiment_runs[0].id)\n",
    "pprint(json.loads(logs))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'engine_specific': None,\n",
      " 'job_logs': {'f11ec9aa-661e-4409-89d6-9ed0005d01f4': {'backend_job_id': 'reana-run-job-7508088f-3536-4f6a-b52d-52678711130b',\n",
      "                                                       'cmd': 'set -a && '\n",
      "                                                              'source .env && '\n",
      "                                                              'set +a && '\n",
      "                                                              'python '\n",
      "                                                              'script.py',\n",
      "                                                       'compute_backend': 'Kubernetes',\n",
      "                                                       'docker_img': 'docker.io/aridzik/rail-exp-templates:template-665998b7d42a40985a8d26a4',\n",
      "                                                       'finished_at': '2024-05-31T10:26:57',\n",
      "                                                       'job_name': 'Execute '\n",
      "                                                                   'Python '\n",
      "                                                                   'script',\n",
      "                                                       'logs': 'job: :\\n'\n",
      "                                                               ' Hello world!\\n'\n",
      "                                                               '\\n'\n",
      "                                                               '\\n'\n",
      "                                                               'Completed\\n',\n",
      "                                                       'started_at': '2024-05-31T10:26:49',\n",
      "                                                       'status': 'finished',\n",
      "                                                       'workflow_uuid': '3fb22d96-690c-4a07-8925-2991d355a9e7'}},\n",
      " 'workflow_logs': '2024-05-31 10:26:49,622 | root | MainThread | INFO | '\n",
      "                  'Publishing step:0, cmd: set -a && source .env && set +a && '\n",
      "                  'python script.py, total steps 1 to MQ\\n'\n",
      "                  '2024-05-31 10:27:01,651 | root | MainThread | INFO | '\n",
      "                  'Workflow 3fb22d96-690c-4a07-8925-2991d355a9e7 finished. '\n",
      "                  'Files available at '\n",
      "                  '/var/reana/users/00000000-0000-0000-0000-000000000000/workflows/3fb22d96-690c-4a07-8925-2991d355a9e7.\\n'\n",
      "                  '\\n'}\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We can also download files from experiment. This call will download script.py into the folder where this code is located"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T10:30:53.522994Z",
     "start_time": "2024-05-31T10:30:52.990320Z"
    }
   },
   "source": "my_client.experiments.download_experiment_run(id=experiment_runs[0].id, filepath=\"script.py\", to_dir=\"run_out\")",
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's archive our experiment which we ran earlier"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T10:30:58.815177Z",
     "start_time": "2024-05-31T10:30:58.798872Z"
    }
   },
   "source": [
    "my_client.experiments.archive(id=experiment_for_run.id, archived=True)"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from output, our experiment MyNewExperiment has been archived"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T10:31:00.997893Z",
     "start_time": "2024-05-31T10:31:00.985044Z"
    }
   },
   "source": [
    "archived_experiments = my_client.experiments.get(archived=True)\n",
    "\n",
    "for a_e in archived_experiments:\n",
    "    print(f\"id: {a_e.id} | name: {a_e.name} | archived: {a_e.archived}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 6659a09cd42a40985a8d26a7 | name: SimpleExperiment | archived: True\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also delete the run which we executed earlier but before that, we have to un-archive the experiment to be able to delete it"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T10:31:13.649572Z",
     "start_time": "2024-05-31T10:31:13.631221Z"
    }
   },
   "source": "my_client.experiments.archive(id=experiment_for_run.id, archived=False)",
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T10:31:53.147037Z",
     "start_time": "2024-05-31T10:31:52.569458Z"
    }
   },
   "source": "my_client.experiments.delete_experiment_run(id=experiment_runs[0].id)",
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the run is deleted"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T10:32:39.267881Z",
     "start_time": "2024-05-31T10:32:39.255073Z"
    }
   },
   "source": "my_client.experiments.get_experiment_runs_count(id=experiment_for_run.id)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiod-rail-sdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
